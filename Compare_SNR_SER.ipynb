{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-14 11:22:45.692708: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-10-14 11:22:45.720356: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-10-14 11:22:45.720387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-10-14 11:22:45.721185: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-10-14 11:22:45.726436: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-10-14 11:22:46.234370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow: 2.15.0\n",
            "GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-14 11:22:47.149110: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-10-14 11:22:47.180163: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-10-14 11:22:47.180210: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n"
          ]
        }
      ],
      "source": [
        "# Lightweight SNRâ€“SER comparison (load UNet, train CAE/DnCNN)\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, Model, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"GPU:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-14 11:22:47.751063: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-10-14 11:22:47.751150: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-10-14 11:22:47.751165: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-10-14 11:22:47.955123: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-10-14 11:22:47.955256: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-10-14 11:22:47.955265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
            "2025-10-14 11:22:47.955304: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-10-14 11:22:47.955321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5556 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data ready: (50000, 32, 32, 3) (10000, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "# Data: CIFAR-10 and z-score helpers\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype('float32')/255.0; y_train = y_train.flatten()\n",
        "x_test  = x_test.astype('float32')/255.0;  y_test  = y_test.flatten()\n",
        "\n",
        "MEAN = tf.constant(np.mean(x_train, axis=(0,1,2)), dtype=tf.float32)\n",
        "STD  = tf.constant(np.std(x_train,  axis=(0,1,2)) + 1e-6, dtype=tf.float32)\n",
        "\n",
        "def to_zscore(x):\n",
        "    return (x - MEAN) / STD\n",
        "\n",
        "def from_zscore(z):\n",
        "    return z * STD + MEAN\n",
        "\n",
        "cifar10_class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "print('Data ready:', x_train.shape, x_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Noise: fixed SNR generators (Gaussian/S&P/Burst)\n",
        "\n",
        "def gaussian_snr_to_cond_vector(snr_db) -> tf.Tensor:\n",
        "    # Fully TF-native: accept tensor or float\n",
        "    snr_db = tf.cast(snr_db, tf.float32)\n",
        "    log10_sigma = -snr_db / 20.0\n",
        "    c = tf.clip_by_value(log10_sigma - 0.5, 0.0, 1.0)\n",
        "    return tf.stack([tf.constant(1.0), tf.constant(0.0), tf.constant(0.0), c])\n",
        "\n",
        "\n",
        "def add_gaussian_noise_fixed_snr(clean_img_01: tf.Tensor, snr_db):\n",
        "    img_z = to_zscore(clean_img_01)\n",
        "    snr_db = tf.cast(snr_db, tf.float32)\n",
        "    sigma = tf.pow(10.0, -snr_db/20.0)\n",
        "    noise = tf.random.normal(tf.shape(img_z), stddev=sigma, dtype=tf.float32)\n",
        "    noisy_z = img_z + noise\n",
        "    cond = gaussian_snr_to_cond_vector(snr_db)\n",
        "    return noisy_z, cond\n",
        "\n",
        "\n",
        "def snr_scale_noise(clean_z: tf.Tensor, noisy_z: tf.Tensor, target_snr_db: tf.Tensor):\n",
        "    noise = noisy_z - clean_z\n",
        "    px = tf.reduce_mean(tf.square(clean_z))\n",
        "    pn = tf.reduce_mean(tf.square(noise)) + 1e-12\n",
        "    r = tf.pow(10.0, target_snr_db/10.0)\n",
        "    pn_target = px / r\n",
        "    k = tf.sqrt(tf.maximum(pn_target / pn, 1e-12))\n",
        "    return clean_z + k*noise\n",
        "\n",
        "\n",
        "def add_sp_noise_fixed_snr(clean_img_01: tf.Tensor, snr_db: tf.Tensor, amount: float = 0.15):\n",
        "    img_z = to_zscore(clean_img_01)\n",
        "    u = tf.random.uniform(tf.shape(img_z))\n",
        "    salt = tf.cast(u < amount*0.5, tf.float32)\n",
        "    pepper = tf.cast(u > 1.0 - amount*0.5, tf.float32)\n",
        "    noisy_z = img_z * (1.0 - salt - pepper) + salt\n",
        "    noisy_z = snr_scale_noise(img_z, noisy_z, snr_db)\n",
        "    return noisy_z, tf.convert_to_tensor([0.0,1.0,0.0,amount], dtype=tf.float32)\n",
        "\n",
        "\n",
        "def add_burst_noise_fixed_snr(clean_img_01: tf.Tensor, snr_db: tf.Tensor, size_factor: float = 0.3, intensity: float = 0.85):\n",
        "    img_z = to_zscore(clean_img_01)\n",
        "    h = tf.shape(img_z)[0]; w = tf.shape(img_z)[1]; cch = tf.shape(img_z)[2]\n",
        "    bh = tf.maximum(1, tf.cast(tf.cast(h, tf.float32)*size_factor, tf.int32))\n",
        "    bw = tf.maximum(1, tf.cast(tf.cast(w, tf.float32)*size_factor, tf.int32))\n",
        "    sy = tf.random.uniform([], maxval=tf.maximum(1, h-bh), dtype=tf.int32)\n",
        "    sx = tf.random.uniform([], maxval=tf.maximum(1, w-bw), dtype=tf.int32)\n",
        "    patch = tf.random.normal([bh, bw, cch], stddev=intensity)\n",
        "    noise = tf.pad(patch, [[sy, h-sy-bh], [sx, w-sx-bw], [0,0]])\n",
        "    mask  = tf.pad(tf.ones([bh, bw, cch]), [[sy, h-sy-bh], [sx, w-sx-bw], [0,0]])\n",
        "    noisy_z = img_z * (1.0 - mask) + (img_z + noise) * mask\n",
        "    noisy_z = snr_scale_noise(img_z, noisy_z, snr_db)\n",
        "    c = tf.clip_by_value(size_factor*intensity, 0.0, 1.0)\n",
        "    return noisy_z, tf.convert_to_tensor([0.0,0.0,1.0,c], dtype=tf.float32)\n",
        "\n",
        "\n",
        "def make_fixed_snr_dataset_noise(x, y, snr_db: float, noise_type: str = 'gaussian', batch_size: int = 128):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    def _map_fn(clean_img, label):\n",
        "        clean_img = tf.cast(clean_img, tf.float32)\n",
        "        sdb = tf.cast(snr_db, tf.float32)\n",
        "        if noise_type == 'gaussian': noisy_z, cond = add_gaussian_noise_fixed_snr(clean_img, sdb)\n",
        "        elif noise_type in ('sp','s&p'): noisy_z, cond = add_sp_noise_fixed_snr(clean_img, sdb)\n",
        "        elif noise_type == 'burst': noisy_z, cond = add_burst_noise_fixed_snr(clean_img, sdb)\n",
        "        else: raise ValueError(noise_type)\n",
        "        clean_z = to_zscore(clean_img)\n",
        "        return (noisy_z, cond), (clean_z, label)\n",
        "    return ds.map(_map_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "No file or directory found at /best_cifar10_conditional_model.keras",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load pretrained UNet (conditional multitask UNet)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m unet_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/best_cifar10_conditional_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m unet_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded UNet from:\u001b[39m\u001b[38;5;124m'\u001b[39m, unet_path)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNet inputs:\u001b[39m\u001b[38;5;124m'\u001b[39m, [inp\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m unet_model\u001b[38;5;241m.\u001b[39minputs])\n",
            "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.11/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.11/site-packages/keras/src/saving/legacy/save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    240\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    241\u001b[0m         )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at /best_cifar10_conditional_model.keras"
          ]
        }
      ],
      "source": [
        "# Load pretrained UNet (conditional multitask UNet)\n",
        "unet_path = 'best_cifar10_conditional_model.keras'\n",
        "unet_model = tf.keras.models.load_model(unet_path)\n",
        "print('Loaded UNet from:', unet_path)\n",
        "print('UNet inputs:', [inp.name for inp in unet_model.inputs])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CAE/DnCNN multitask models\n",
        "\n",
        "def build_cae_multitask(input_shape_img=(32,32,3), input_shape_map=(4,), num_classes=10):\n",
        "    img_in  = layers.Input(shape=input_shape_img, name='image_input')\n",
        "    cond_in = layers.Input(shape=input_shape_map, name='noise_map_input')\n",
        "    x = layers.Conv2D(32,3,padding='same',activation='relu')(img_in)\n",
        "    x = layers.Conv2D(32,3,padding='same',activation='relu')(x)\n",
        "    s1 = x  # 32x32\n",
        "    p1 = layers.MaxPooling2D(2)(s1)  # 16x16\n",
        "    x = layers.Conv2D(64,3,padding='same',activation='relu')(p1)\n",
        "    x = layers.Conv2D(64,3,padding='same',activation='relu')(x)\n",
        "    s2 = x  # 16x16\n",
        "    p2 = layers.MaxPooling2D(2)(s2)  # 8x8\n",
        "    x = layers.Conv2D(128,3,padding='same',activation='relu')(p2)\n",
        "    x = layers.Conv2D(128,3,padding='same',activation='relu')(x)\n",
        "    feat = layers.GlobalAveragePooling2D()(x)\n",
        "    feat = layers.Concatenate()([feat, cond_in])\n",
        "    feat = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(feat)\n",
        "    feat = layers.Dropout(0.5)(feat)\n",
        "    cls_out = layers.Dense(num_classes, activation='softmax', name='classification_output')(feat)\n",
        "    d = layers.Conv2DTranspose(64,2,strides=2,padding='same')(x)  # 16x16\n",
        "    d = layers.Concatenate()([d, s2])\n",
        "    d = layers.Conv2D(64,3,padding='same',activation='relu')(d)\n",
        "    d = layers.Conv2D(64,3,padding='same',activation='relu')(d)\n",
        "    d = layers.Conv2DTranspose(32,2,strides=2,padding='same')(d)  # 32x32\n",
        "    d = layers.Concatenate()([d, s1])\n",
        "    d = layers.Conv2D(32,3,padding='same',activation='relu')(d)\n",
        "    d = layers.Conv2D(32,3,padding='same',activation='relu')(d)\n",
        "    rec = layers.Conv2D(3,1,activation='linear', name='restoration_output')(d)\n",
        "    return Model(inputs=[img_in, cond_in], outputs=[rec, cls_out], name='CAE_multitask')\n",
        "\n",
        "\n",
        "def build_dncnn_multitask(input_shape_img=(32,32,3), input_shape_map=(4,), num_classes=10, depth=17, filters=64):\n",
        "    img_in  = layers.Input(shape=input_shape_img, name='image_input')\n",
        "    cond_in = layers.Input(shape=input_shape_map, name='noise_map_input')\n",
        "    x = layers.Conv2D(filters,3,padding='same',activation='relu')(img_in)\n",
        "    for _ in range(depth-2):\n",
        "        x = layers.Conv2D(filters,3,padding='same',use_bias=False)(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('relu')(x)\n",
        "    res = layers.Conv2D(3,3,padding='same',activation='linear', name='residual_pred')(x)\n",
        "    rec = layers.Subtract(name='restoration_output')([img_in, res])\n",
        "    feat = layers.GlobalAveragePooling2D()(x)\n",
        "    feat = layers.Concatenate()([feat, cond_in])\n",
        "    feat = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(feat)\n",
        "    feat = layers.Dropout(0.5)(feat)\n",
        "    cls_out = layers.Dense(num_classes, activation='softmax', name='classification_output')(feat)\n",
        "    return Model(inputs=[img_in, cond_in], outputs=[rec, cls_out], name='DnCNN_multitask')\n",
        "\n",
        "# Learning rate schedule (match UNet): ExponentialDecay per-step\n",
        "STEPS_PER_EPOCH = int(np.ceil(len(x_train)/128))\n",
        "initial_learning_rate = 1e-4\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=STEPS_PER_EPOCH,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "cae_model = build_cae_multitask(num_classes=10)\n",
        "dncnn_model = build_dncnn_multitask(num_classes=10)\n",
        "\n",
        "for m in [cae_model, dncnn_model]:\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "              loss={'restoration_output':'mae','classification_output':'sparse_categorical_crossentropy'},\n",
        "              loss_weights={'restoration_output':0.8,'classification_output':0.2},\n",
        "              metrics={'classification_output':'accuracy'})\n",
        "\n",
        "print('CAE/DnCNN ready with LR schedule')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mixed-SNR training dataset (Gaussian only by default)\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def gen_mixed_gaussian_sample(clean_img, label):\n",
        "    clean_img = tf.cast(clean_img, tf.float32)\n",
        "    snr_db = tf.random.uniform([], -30.0, -10.0)\n",
        "    noisy_z, cond = add_gaussian_noise_fixed_snr(clean_img, snr_db)\n",
        "    clean_z = to_zscore(clean_img)\n",
        "    return (noisy_z, cond), (clean_z, label)\n",
        "\n",
        "train_ds_mixed = (tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "                  .shuffle(50000)\n",
        "                  .map(gen_mixed_gaussian_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                  .batch(BATCH_SIZE)\n",
        "                  .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "val_ds_mixed = (tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "                .map(gen_mixed_gaussian_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "print('Datasets ready')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SER evaluators and plotting\n",
        "\n",
        "def eval_model_ser_over_snrs(model, x, y, snr_list_db, noise_type='gaussian', batch_size=512):\n",
        "    results = {}\n",
        "    for snr in snr_list_db:\n",
        "        ds = make_fixed_snr_dataset_noise(x, y, snr_db=float(snr), noise_type=noise_type, batch_size=batch_size)\n",
        "        total = 0; errors = 0\n",
        "        for (noisy_z_b, cond_b), (clean_z_b, label_b) in ds:\n",
        "            _, logits_b = model.predict([noisy_z_b, cond_b], verbose=0)\n",
        "            pred = np.argmax(logits_b, axis=-1)\n",
        "            total += label_b.shape[0]\n",
        "            errors += int(np.sum(pred != label_b.numpy()))\n",
        "        results[float(snr)] = errors / max(1, total)\n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_snr_ser(models_ser_dict, title='SNR vs SER', threshold=0.10):\n",
        "    plt.figure(figsize=(7,5))\n",
        "    for name, ser_map in models_ser_dict.items():\n",
        "        snrs = np.array(sorted(ser_map.keys()))\n",
        "        sers = np.array([ser_map[s] for s in snrs])\n",
        "        plt.plot(snrs, sers, marker='o', label=name)\n",
        "        idx = np.where(np.diff((sers <= threshold).astype(int)) != 0)[0]\n",
        "        if idx.size > 0:\n",
        "            i = idx[0]\n",
        "            x0,x1 = snrs[i], snrs[i+1]; y0,y1 = sers[i], sers[i+1]\n",
        "            if y1 != y0:\n",
        "                x_cross = x0 + (threshold - y0) * (x1 - x0) / (y1 - y0)\n",
        "                plt.scatter([x_cross],[threshold], marker='x', s=80)\n",
        "                plt.text(x_cross, threshold+0.02, f\"{name}: {x_cross:.1f} dB\", ha='center', fontsize=9)\n",
        "    plt.axhline(threshold, color='gray', ls='--', lw=1, label='SER=0.10')\n",
        "    plt.ylim(0,1); plt.xlabel('SNR (dB)'); plt.ylabel('SER'); plt.title(title); plt.grid(ls=':'); plt.legend(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate UNet SER first (Gaussian)\n",
        "snr_grid = list(range(-30, -9, 2))\n",
        "unet_ser = eval_model_ser_over_snrs(unet_model, x_test, y_test, snr_grid, noise_type='gaussian', batch_size=512)\n",
        "print('UNet SER computed for', len(snr_grid), 'SNR points')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/Load CAE MTL only, then evaluate SER (Gaussian)\n",
        "EPOCHS = 200\n",
        "cae_ckpt = 'best_cae_multitask.keras'\n",
        "\n",
        "\n",
        "callbacks_cae = [\n",
        "    EarlyStopping(monitor='val_classification_output_accuracy', patience=20, restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=cae_ckpt, save_weights_only=False, monitor='val_classification_output_accuracy', mode='max', save_best_only=True)\n",
        "]\n",
        "\n",
        "if os.path.exists(cae_ckpt):\n",
        "    print(f'Loading CAE weights from {cae_ckpt}')\n",
        "    cae_model = tf.keras.models.load_model(cae_ckpt)\n",
        "else:\n",
        "    print('\\nTraining CAE (up to 200 epochs, early-stop) ...')\n",
        "    cae_model.fit(train_ds_mixed, epochs=EPOCHS, validation_data=val_ds_mixed, callbacks=callbacks_cae, verbose=1)\n",
        "\n",
        "cae_ser = eval_model_ser_over_snrs(cae_model, x_test, y_test, snr_grid, noise_type='gaussian', batch_size=512)\n",
        "\n",
        "models_map = {'UNet (MTL)': unet_ser, 'CAE (MTL)': cae_ser}\n",
        "plot_snr_ser(models_map, title='Gaussian: SNR vs SER (UNet vs CAE)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fixed classifier (clean CIFAR-10) with load-if-exists\n",
        "\n",
        "def build_fixed_classifier(input_shape=(32,32,3), num_classes=10):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
        "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    return Model(inputs, outputs, name='FixedClassifier')\n",
        "\n",
        "clf_ckpt = '/best_fixed_classifier.keras'\n",
        "if os.path.exists(clf_ckpt):\n",
        "    print(f'Loading fixed classifier from {clf_ckpt}')\n",
        "    fixed_clf = tf.keras.models.load_model(clf_ckpt)\n",
        "else:\n",
        "    fixed_clf = build_fixed_classifier()\n",
        "    fixed_clf.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "    clf_callbacks = [\n",
        "        EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True),\n",
        "        ModelCheckpoint(filepath=clf_ckpt, save_weights_only=False, monitor='val_accuracy', mode='max', save_best_only=True)\n",
        "    ]\n",
        "    print('\\nTraining fixed classifier on clean CIFAR-10 ...')\n",
        "    fixed_clf.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
        "                  epochs=50, batch_size=256, callbacks=clf_callbacks, verbose=1)\n",
        "    print('Saved best fixed classifier to', clf_ckpt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic restoration-only CAE/DnCNN (no conditioning, no classifier head)\n",
        "\n",
        "def build_cae_restoration(input_shape=(32,32,3)):\n",
        "    inp = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32,3,padding='same',activation='relu')(inp)\n",
        "    x = layers.Conv2D(32,3,padding='same',activation='relu')(x)\n",
        "    s1 = x\n",
        "    p1 = layers.MaxPooling2D(2)(x)\n",
        "    x = layers.Conv2D(64,3,padding='same',activation='relu')(p1)\n",
        "    x = layers.Conv2D(64,3,padding='same',activation='relu')(x)\n",
        "    s2 = x\n",
        "    p2 = layers.MaxPooling2D(2)(x)\n",
        "    x = layers.Conv2D(128,3,padding='same',activation='relu')(p2)\n",
        "    x = layers.Conv2D(128,3,padding='same',activation='relu')(x)\n",
        "    d = layers.Conv2DTranspose(64,2,strides=2,padding='same')(x)\n",
        "    d = layers.Concatenate()([d, s2])\n",
        "    d = layers.Conv2D(64,3,padding='same',activation='relu')(d)\n",
        "    d = layers.Conv2D(64,3,padding='same',activation='relu')(d)\n",
        "    d = layers.Conv2DTranspose(32,2,strides=2,padding='same')(d)\n",
        "    d = layers.Concatenate()([d, s1])\n",
        "    d = layers.Conv2D(32,3,padding='same',activation='relu')(d)\n",
        "    d = layers.Conv2D(32,3,padding='same',activation='relu')(d)\n",
        "    out = layers.Conv2D(3,1,activation='linear')(d)\n",
        "    return Model(inp, out, name='CAE_restoration')\n",
        "\n",
        "\n",
        "def build_dncnn_restoration(input_shape=(32,32,3), depth=17, filters=64):\n",
        "    inp = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(filters,3,padding='same',activation='relu')(inp)\n",
        "    for _ in range(depth-2):\n",
        "        x = layers.Conv2D(filters,3,padding='same',use_bias=False)(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('relu')(x)\n",
        "    res = layers.Conv2D(3,3,padding='same',activation='linear')(x)\n",
        "    out = layers.Subtract()([inp, res])\n",
        "    return Model(inp, out, name='DnCNN_restoration')\n",
        "\n",
        "cae_rest_ckpt = '/best_cae_restoration.keras'\n",
        "dncnn_rest_ckpt = '/best_dncnn_restoration.keras'\n",
        "\n",
        "cae_rest = build_cae_restoration(); dncnn_rest = build_dncnn_restoration()\n",
        "for m in [cae_rest, dncnn_rest]:\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='mae')\n",
        "\n",
        "# Train or load restoration-only models using mixed Gaussian SNR data\n",
        "rest_train = train_ds_mixed.map(lambda inp, tgt: (from_zscore(inp[0]), from_zscore(tgt[0])))\n",
        "rest_val   = val_ds_mixed.map(lambda inp, tgt: (from_zscore(inp[0]), from_zscore(tgt[0])))\n",
        "\n",
        "rest_callbacks_cae = [EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "                      ModelCheckpoint(filepath=cae_rest_ckpt, save_weights_only=False, monitor='val_loss', mode='min', save_best_only=True)]\n",
        "rest_callbacks_dn  = [EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "                      ModelCheckpoint(filepath=dncnn_rest_ckpt, save_weights_only=False, monitor='val_loss', mode='min', save_best_only=True)]\n",
        "\n",
        "if os.path.exists(cae_rest_ckpt):\n",
        "    print(f'Loading CAE restoration from {cae_rest_ckpt}')\n",
        "    cae_rest = tf.keras.models.load_model(cae_rest_ckpt)\n",
        "else:\n",
        "    print('\\nTraining CAE restoration ...')\n",
        "    cae_rest.fit(rest_train, validation_data=rest_val, epochs=100, callbacks=rest_callbacks_cae, verbose=1)\n",
        "\n",
        "if os.path.exists(dncnn_rest_ckpt):\n",
        "    print(f'Loading DnCNN restoration from {dncnn_rest_ckpt}')\n",
        "    dncnn_rest = tf.keras.models.load_model(dncnn_rest_ckpt)\n",
        "else:\n",
        "    print('\\nTraining DnCNN restoration ...')\n",
        "    dncnn_rest.fit(rest_train, validation_data=rest_val, epochs=100, callbacks=rest_callbacks_dn, verbose=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline SER evaluators (no-rest and restoration+classifier)\n",
        "\n",
        "def eval_pipeline_ser_over_snrs(classifier, restorer, x, y, snr_list_db, noise_type='gaussian', batch_size=512):\n",
        "    results = {}\n",
        "    for snr in snr_list_db:\n",
        "        ds = make_fixed_snr_dataset_noise(x, y, snr_db=float(snr), noise_type=noise_type, batch_size=batch_size)\n",
        "        total = 0; errors = 0\n",
        "        for (noisy_z_b, cond_b), (clean_z_b, label_b) in ds:\n",
        "            if restorer is None:\n",
        "                restored = from_zscore(noisy_z_b)\n",
        "            else:\n",
        "                restored = restorer.predict(from_zscore(noisy_z_b), verbose=0)\n",
        "            logits_b = classifier.predict(restored, verbose=0)\n",
        "            pred = np.argmax(logits_b, axis=-1)\n",
        "            total += label_b.shape[0]\n",
        "            errors += int(np.sum(pred != label_b.numpy()))\n",
        "        results[float(snr)] = errors / max(1, total)\n",
        "    return results\n",
        "\n",
        "snr_grid = list(range(-30, -9, 2))\n",
        "\n",
        "# Evaluate pipeline baselines (Gaussian)\n",
        "no_rest_ser  = eval_pipeline_ser_over_snrs(fixed_clf, None,       x_test, y_test, snr_grid, noise_type='gaussian', batch_size=512)\n",
        "cae_pipe_ser = eval_pipeline_ser_over_snrs(fixed_clf, cae_rest,   x_test, y_test, snr_grid, noise_type='gaussian', batch_size=512)\n",
        "dn_pipe_ser  = eval_pipeline_ser_over_snrs(fixed_clf, dncnn_rest, x_test, y_test, snr_grid, noise_type='gaussian', batch_size=512)\n",
        "\n",
        "# Combine plots with MTL models already computed in previous cell\n",
        "models_map = {\n",
        "    'UNet (MTL)': unet_ser,\n",
        "    'CAE (MTL)': cae_ser,\n",
        "    'DnCNN (MTL)': dncnn_ser,\n",
        "    'No-Rest + FixedClf': no_rest_ser,\n",
        "    'CAE-Rest + FixedClf': cae_pipe_ser,\n",
        "    'DnCNN-Rest + FixedClf': dn_pipe_ser,   \n",
        "}\n",
        "plot_snr_ser(models_map, title='Gaussian: SNR vs SER (MTL vs Pipelines)')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/Load DnCNN MTL separately, then evaluate SER (Gaussian)\n",
        "EPOCHS = 200\n",
        "dncnn_ckpt = 'best_dncnn_multitask.keras'\n",
        "\n",
        "callbacks_dncnn = [\n",
        "    EarlyStopping(monitor='val_classification_output_accuracy', patience=20, restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=dncnn_ckpt, save_weights_only=False, monitor='val_classification_output_accuracy', mode='max', save_best_only=True)\n",
        "]\n",
        "\n",
        "if os.path.exists(dncnn_ckpt):\n",
        "    print(f'Loading DnCNN weights from {dncnn_ckpt}')\n",
        "    dncnn_model = tf.keras.models.load_model(dncnn_ckpt)\n",
        "else:\n",
        "    print('\\nTraining DnCNN (up to 200 epochs, early-stop) ...')\n",
        "    dncnn_model.fit(train_ds_mixed, epochs=EPOCHS, validation_data=val_ds_mixed, callbacks=callbacks_dncnn, verbose=1)\n",
        "\n",
        "try:\n",
        "    dncnn_ser = eval_model_ser_over_snrs(dncnn_model, x_test, y_test, snr_grid, noise_type='gaussian', batch_size=512)\n",
        "    models_map = {'UNet (MTL)': unet_ser}\n",
        "    if 'cae_ser' in globals(): models_map['CAE (MTL)'] = cae_ser\n",
        "    models_map['DnCNN (MTL)'] = dncnn_ser\n",
        "    plot_snr_ser(models_map, title='Gaussian: SNR vs SER (UNet vs CAE vs DnCNN)')\n",
        "except Exception as e:\n",
        "    print('DnCNN evaluation skipped due to error:', e)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
