{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Detailed Performance Analysis\n",
    "\n",
    "## Noise Type & SNR Level Analysis\n",
    "\n",
    "This notebook performs in-depth analysis by:\n",
    "- Noise type (Gaussian, Salt & Pepper, Burst)\n",
    "- SNR level (-30, -25, -20, -15, -10 dB)\n",
    "- Model comparison across different conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set matplotlib font to support English\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "x_test_augmented = np.load('data/x_test_augmented.npy')\n",
    "y_test_augmented = np.load('data/y_test_augmented.npy')\n",
    "x_test_clean = np.load('data/x_test_clean.npy')\n",
    "test_noise_info = pd.read_csv('data/test_noise_info.csv')\n",
    "\n",
    "print(f\"Test data shape: {x_test_augmented.shape}\")\n",
    "print(f\"Test noise info shape: {test_noise_info.shape}\")\n",
    "\n",
    "# Prepare clean data for comparison (repeat 3 times for 3 noise types)\n",
    "x_test_clean_repeated = np.repeat(x_test_clean, 3, axis=0)\n",
    "\n",
    "# For BAM models (flattened)\n",
    "x_test_flat = x_test_augmented.reshape(x_test_augmented.shape[0], -1)\n",
    "x_test_clean_flat = x_test_clean_repeated.reshape(x_test_clean_repeated.shape[0], -1)\n",
    "\n",
    "print(\"\\nData loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading trained models...\")\n",
    "\n",
    "# Model configurations\n",
    "model_configs = [\n",
    "    {\n",
    "        'name': 'Sequential BAM',\n",
    "        'type': 'sequential',\n",
    "        'arch': 'BAM',\n",
    "        'restore_path': 'weights/sequential_bam_denoise.keras',\n",
    "        'cls_path': 'weights/sequential_bam_classification.keras',\n",
    "        'input_type': 'flat'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Sequential CAE',\n",
    "        'type': 'sequential',\n",
    "        'arch': 'CAE',\n",
    "        'restore_path': 'weights/sequential_cae_restore.keras',\n",
    "        'cls_path': 'weights/sequential_cae_classification.keras',\n",
    "        'input_type': 'image'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Sequential U-Net',\n",
    "        'type': 'sequential',\n",
    "        'arch': 'U-Net',\n",
    "        'restore_path': 'weights/sequential_unet_restore.keras',\n",
    "        'cls_path': 'weights/sequential_unet_classification.keras',\n",
    "        'input_type': 'image'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MTL BAM',\n",
    "        'type': 'mtl',\n",
    "        'arch': 'BAM',\n",
    "        'model_path': 'weights/mtl_bam.keras',\n",
    "        'input_type': 'flat'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MTL CAE',\n",
    "        'type': 'mtl',\n",
    "        'arch': 'CAE',\n",
    "        'model_path': 'weights/mtl_cae.keras',\n",
    "        'input_type': 'image'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MTL U-Net',\n",
    "        'type': 'mtl',\n",
    "        'arch': 'U-Net',\n",
    "        'model_path': 'weights/mtl_unet.keras',\n",
    "        'input_type': 'image'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Load models\n",
    "loaded_models = {}\n",
    "for config in model_configs:\n",
    "    try:\n",
    "        if config['type'] == 'sequential':\n",
    "            restore_model = keras.models.load_model(config['restore_path'])\n",
    "            cls_model = keras.models.load_model(config['cls_path'])\n",
    "            loaded_models[config['name']] = {\n",
    "                'restore': restore_model,\n",
    "                'classify': cls_model,\n",
    "                'type': 'sequential',\n",
    "                'arch': config['arch'],\n",
    "                'input_type': config['input_type']\n",
    "            }\n",
    "            print(f\"✓ {config['name']} loaded (Sequential)\")\n",
    "        else:  # MTL\n",
    "            model = keras.models.load_model(config['model_path'])\n",
    "            loaded_models[config['name']] = {\n",
    "                'model': model,\n",
    "                'type': 'mtl',\n",
    "                'arch': config['arch'],\n",
    "                'input_type': config['input_type']\n",
    "            }\n",
    "            print(f\"✓ {config['name']} loaded (MTL)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {config['name']} failed to load: {e}\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(loaded_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(img1, img2):\n",
    "    \"\"\"Calculate PSNR\"\"\"\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "\n",
    "def evaluate_by_condition(model_dict, x_test, x_clean, y_test, condition_mask, batch_size=128):\n",
    "    \"\"\"\n",
    "    Evaluate model on specific condition (noise type or SNR level)\n",
    "    \n",
    "    Args:\n",
    "        model_dict: Model dictionary\n",
    "        x_test: Test noisy images\n",
    "        x_clean: Clean reference images\n",
    "        y_test: Test labels\n",
    "        condition_mask: Boolean mask for specific condition\n",
    "        batch_size: Batch size for prediction\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "    # Filter data by condition\n",
    "    x_cond = x_test[condition_mask]\n",
    "    x_clean_cond = x_clean[condition_mask]\n",
    "    y_cond = y_test[condition_mask]\n",
    "    \n",
    "    if len(x_cond) == 0:\n",
    "        return None\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if model_dict['type'] == 'sequential':\n",
    "        # Stage 1: Restoration\n",
    "        restored = model_dict['restore'].predict(x_cond, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        # Restoration metrics\n",
    "        mse = np.mean((restored - x_clean_cond) ** 2)\n",
    "        mae = np.mean(np.abs(restored - x_clean_cond))\n",
    "        psnr = calculate_psnr(restored, x_clean_cond)\n",
    "        \n",
    "        results['restoration'] = {\n",
    "            'mse': float(mse),\n",
    "            'mae': float(mae),\n",
    "            'psnr': float(psnr)\n",
    "        }\n",
    "        \n",
    "        # Stage 2: Classification\n",
    "        predictions = model_dict['classify'].predict(x_cond, batch_size=batch_size, verbose=0)\n",
    "        pred_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(y_cond, axis=1)\n",
    "        accuracy = np.mean(pred_classes == true_classes)\n",
    "        \n",
    "        results['classification'] = {\n",
    "            'accuracy': float(accuracy)\n",
    "        }\n",
    "        \n",
    "    else:  # MTL\n",
    "        # Get both outputs\n",
    "        predictions = model_dict['model'].predict(x_cond, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        if isinstance(predictions, dict):\n",
    "            restored = predictions['restoration_output']\n",
    "            cls_output = predictions['classification_output']\n",
    "        else:\n",
    "            restored, cls_output = predictions\n",
    "        \n",
    "        # Restoration metrics\n",
    "        mse = np.mean((restored - x_clean_cond) ** 2)\n",
    "        mae = np.mean(np.abs(restored - x_clean_cond))\n",
    "        psnr = calculate_psnr(restored, x_clean_cond)\n",
    "        \n",
    "        results['restoration'] = {\n",
    "            'mse': float(mse),\n",
    "            'mae': float(mae),\n",
    "            'psnr': float(psnr)\n",
    "        }\n",
    "        \n",
    "        # Classification metrics\n",
    "        pred_classes = np.argmax(cls_output, axis=1)\n",
    "        true_classes = np.argmax(y_cond, axis=1)\n",
    "        accuracy = np.mean(pred_classes == true_classes)\n",
    "        \n",
    "        results['classification'] = {\n",
    "            'accuracy': float(accuracy)\n",
    "        }\n",
    "    \n",
    "    results['num_samples'] = len(x_cond)\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate by Noise Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Evaluating by Noise Type\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "noise_types = ['gaussian', 'sp', 'burst']\n",
    "noise_results = {}\n",
    "\n",
    "for model_name, model_dict in loaded_models.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    noise_results[model_name] = {}\n",
    "    \n",
    "    # Select appropriate input\n",
    "    if model_dict['input_type'] == 'flat':\n",
    "        x_input = x_test_flat\n",
    "        x_clean_input = x_test_clean_flat\n",
    "    else:\n",
    "        x_input = x_test_augmented\n",
    "        x_clean_input = x_test_clean_repeated\n",
    "    \n",
    "    for noise_type in noise_types:\n",
    "        # Create mask for this noise type\n",
    "        mask = test_noise_info['noise_type'] == noise_type\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_by_condition(\n",
    "            model_dict, x_input, x_clean_input, y_test_augmented, mask\n",
    "        )\n",
    "        \n",
    "        if results:\n",
    "            noise_results[model_name][noise_type] = results\n",
    "            print(f\"  {noise_type:10s}: PSNR={results['restoration']['psnr']:.2f}dB, \"\n",
    "                  f\"Acc={results['classification']['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Noise type evaluation complete!\")\n",
    "\n",
    "# Save results\n",
    "with open('results/noise_type_results.json', 'w') as f:\n",
    "    json.dump(noise_results, f, indent=2)\n",
    "print(\"✓ Results saved: results/noise_type_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate by SNR Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Evaluating by SNR Level\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "snr_levels = [-30, -25, -20, -15, -10]\n",
    "snr_results = {}\n",
    "\n",
    "for model_name, model_dict in loaded_models.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    snr_results[model_name] = {}\n",
    "    \n",
    "    # Select appropriate input\n",
    "    if model_dict['input_type'] == 'flat':\n",
    "        x_input = x_test_flat\n",
    "        x_clean_input = x_test_clean_flat\n",
    "    else:\n",
    "        x_input = x_test_augmented\n",
    "        x_clean_input = x_test_clean_repeated\n",
    "    \n",
    "    for snr in snr_levels:\n",
    "        # Create mask for this SNR level\n",
    "        mask = test_noise_info['snr_db'] == snr\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_by_condition(\n",
    "            model_dict, x_input, x_clean_input, y_test_augmented, mask\n",
    "        )\n",
    "        \n",
    "        if results:\n",
    "            snr_results[model_name][str(snr)] = results\n",
    "            print(f\"  {snr:3d}dB: PSNR={results['restoration']['psnr']:.2f}dB, \"\n",
    "                  f\"Acc={results['classification']['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ SNR level evaluation complete!\")\n",
    "\n",
    "# Save results\n",
    "with open('results/snr_level_results.json', 'w') as f:\n",
    "    json.dump(snr_results, f, indent=2)\n",
    "print(\"✓ Results saved: results/snr_level_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Type Summary Table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOISE TYPE PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "noise_summary_data = []\n",
    "for model_name in loaded_models.keys():\n",
    "    for noise_type in noise_types:\n",
    "        if noise_type in noise_results[model_name]:\n",
    "            result = noise_results[model_name][noise_type]\n",
    "            noise_summary_data.append({\n",
    "                'Model': model_name,\n",
    "                'Noise Type': noise_type.upper(),\n",
    "                'PSNR (dB)': result['restoration']['psnr'],\n",
    "                'MSE': result['restoration']['mse'],\n",
    "                'MAE': result['restoration']['mae'],\n",
    "                'Accuracy': result['classification']['accuracy'],\n",
    "                'Samples': result['num_samples']\n",
    "            })\n",
    "\n",
    "noise_summary_df = pd.DataFrame(noise_summary_data)\n",
    "print(noise_summary_df.to_string(index=False))\n",
    "noise_summary_df.to_csv('results/noise_type_summary.csv', index=False)\n",
    "print(\"\\n✓ Saved: results/noise_type_summary.csv\")\n",
    "\n",
    "# SNR Level Summary Table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SNR LEVEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "snr_summary_data = []\n",
    "for model_name in loaded_models.keys():\n",
    "    for snr in snr_levels:\n",
    "        snr_key = str(snr)\n",
    "        if snr_key in snr_results[model_name]:\n",
    "            result = snr_results[model_name][snr_key]\n",
    "            snr_summary_data.append({\n",
    "                'Model': model_name,\n",
    "                'SNR (dB)': snr,\n",
    "                'PSNR (dB)': result['restoration']['psnr'],\n",
    "                'MSE': result['restoration']['mse'],\n",
    "                'MAE': result['restoration']['mae'],\n",
    "                'Accuracy': result['classification']['accuracy'],\n",
    "                'Samples': result['num_samples']\n",
    "            })\n",
    "\n",
    "snr_summary_df = pd.DataFrame(snr_summary_data)\n",
    "print(snr_summary_df.to_string(index=False))\n",
    "snr_summary_df.to_csv('results/snr_level_summary.csv', index=False)\n",
    "print(\"\\n✓ Saved: results/snr_level_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization: Noise Type Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Performance by Noise Type', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Color scheme\n",
    "seq_color = '#2E86AB'\n",
    "mtl_color = '#A23B72'\n",
    "colors_dict = {\n",
    "    'Sequential BAM': seq_color,\n",
    "    'Sequential CAE': seq_color,\n",
    "    'Sequential U-Net': seq_color,\n",
    "    'MTL BAM': mtl_color,\n",
    "    'MTL CAE': mtl_color,\n",
    "    'MTL U-Net': mtl_color\n",
    "}\n",
    "\n",
    "# Plot 1: PSNR by Noise Type\n",
    "ax = axes[0, 0]\n",
    "for model_name in loaded_models.keys():\n",
    "    psnr_values = [noise_results[model_name][nt]['restoration']['psnr'] \n",
    "                   for nt in noise_types if nt in noise_results[model_name]]\n",
    "    ax.plot(noise_types, psnr_values, marker='o', linewidth=2, \n",
    "            label=model_name, color=colors_dict[model_name], markersize=8)\n",
    "ax.set_xlabel('Noise Type', fontsize=12)\n",
    "ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "ax.set_title('Restoration PSNR', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: MSE by Noise Type\n",
    "ax = axes[0, 1]\n",
    "for model_name in loaded_models.keys():\n",
    "    mse_values = [noise_results[model_name][nt]['restoration']['mse'] \n",
    "                  for nt in noise_types if nt in noise_results[model_name]]\n",
    "    ax.plot(noise_types, mse_values, marker='o', linewidth=2, \n",
    "            label=model_name, color=colors_dict[model_name], markersize=8)\n",
    "ax.set_xlabel('Noise Type', fontsize=12)\n",
    "ax.set_ylabel('MSE', fontsize=12)\n",
    "ax.set_title('Restoration MSE', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Classification Accuracy by Noise Type\n",
    "ax = axes[0, 2]\n",
    "for model_name in loaded_models.keys():\n",
    "    acc_values = [noise_results[model_name][nt]['classification']['accuracy'] * 100 \n",
    "                  for nt in noise_types if nt in noise_results[model_name]]\n",
    "    ax.plot(noise_types, acc_values, marker='o', linewidth=2, \n",
    "            label=model_name, color=colors_dict[model_name], markersize=8)\n",
    "ax.set_xlabel('Noise Type', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Classification Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Bar chart - PSNR by Noise Type\n",
    "ax = axes[1, 0]\n",
    "x_pos = np.arange(len(noise_types))\n",
    "width = 0.15\n",
    "for i, model_name in enumerate(loaded_models.keys()):\n",
    "    psnr_values = [noise_results[model_name][nt]['restoration']['psnr'] \n",
    "                   for nt in noise_types if nt in noise_results[model_name]]\n",
    "    ax.bar(x_pos + i * width, psnr_values, width, \n",
    "           label=model_name, color=colors_dict[model_name], alpha=0.8)\n",
    "ax.set_xlabel('Noise Type', fontsize=12)\n",
    "ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "ax.set_title('PSNR Comparison (Bar)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos + width * 2.5)\n",
    "ax.set_xticklabels(noise_types)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 5: Bar chart - Accuracy by Noise Type\n",
    "ax = axes[1, 1]\n",
    "for i, model_name in enumerate(loaded_models.keys()):\n",
    "    acc_values = [noise_results[model_name][nt]['classification']['accuracy'] * 100 \n",
    "                  for nt in noise_types if nt in noise_results[model_name]]\n",
    "    ax.bar(x_pos + i * width, acc_values, width, \n",
    "           label=model_name, color=colors_dict[model_name], alpha=0.8)\n",
    "ax.set_xlabel('Noise Type', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Accuracy Comparison (Bar)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos + width * 2.5)\n",
    "ax.set_xticklabels(noise_types)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 6: Heatmap - PSNR by Model and Noise Type\n",
    "ax = axes[1, 2]\n",
    "heatmap_data = []\n",
    "for model_name in loaded_models.keys():\n",
    "    psnr_values = [noise_results[model_name][nt]['restoration']['psnr'] \n",
    "                   for nt in noise_types if nt in noise_results[model_name]]\n",
    "    heatmap_data.append(psnr_values)\n",
    "heatmap_df = pd.DataFrame(heatmap_data, \n",
    "                          index=list(loaded_models.keys()),\n",
    "                          columns=noise_types)\n",
    "sns.heatmap(heatmap_df, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax, cbar_kws={'label': 'PSNR (dB)'})\n",
    "ax.set_title('PSNR Heatmap', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Noise Type', fontsize=12)\n",
    "ax.set_ylabel('Model', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/noise_type_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization saved: results/noise_type_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization: SNR Level Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Performance by SNR Level', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: PSNR vs SNR\n",
    "ax = axes[0, 0]\n",
    "for model_name in loaded_models.keys():\n",
    "    psnr_values = [snr_results[model_name][str(snr)]['restoration']['psnr'] \n",
    "                   for snr in snr_levels if str(snr) in snr_results[model_name]]\n",
    "    ax.plot(snr_levels, psnr_values, marker='o', linewidth=2, \n",
    "            label=model_name, color=colors_dict[model_name], markersize=8)\n",
    "ax.set_xlabel('Input SNR (dB)', fontsize=12)\n",
    "ax.set_ylabel('Output PSNR (dB)', fontsize=12)\n",
    "ax.set_title('Restoration PSNR vs SNR', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: MSE vs SNR\n",
    "ax = axes[0, 1]\n",
    "for model_name in loaded_models.keys():\n",
    "    mse_values = [snr_results[model_name][str(snr)]['restoration']['mse'] \n",
    "                  for snr in snr_levels if str(snr) in snr_results[model_name]]\n",
    "    ax.plot(snr_levels, mse_values, marker='o', linewidth=2, \n",
    "            label=model_name, color=colors_dict[model_name], markersize=8)\n",
    "ax.set_xlabel('Input SNR (dB)', fontsize=12)\n",
    "ax.set_ylabel('MSE', fontsize=12)\n",
    "ax.set_title('Restoration MSE vs SNR', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Classification Accuracy vs SNR\n",
    "ax = axes[0, 2]\n",
    "for model_name in loaded_models.keys():\n",
    "    acc_values = [snr_results[model_name][str(snr)]['classification']['accuracy'] * 100 \n",
    "                  for snr in snr_levels if str(snr) in snr_results[model_name]]\n",
    "    ax.plot(snr_levels, acc_values, marker='o', linewidth=2, \n",
    "            label=model_name, color=colors_dict[model_name], markersize=8)\n",
    "ax.set_xlabel('Input SNR (dB)', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Classification Accuracy vs SNR', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: PSNR Improvement over SNR\n",
    "ax = axes[1, 0]\n",
    "for model_name in loaded_models.keys():\n",
    "    psnr_values = [snr_results[model_name][str(snr)]['restoration']['psnr'] \n",
    "                   for snr in snr_levels if str(snr) in snr_results[model_name]]\n",
    "    # Calculate improvement (Output PSNR - Input SNR)\n",
    "    improvements = [psnr - snr for psnr, snr in zip(psnr_values, snr_levels)]\n",
    "    ax.plot(snr_levels, improvements, marker='o', linewidth=2, \n",
    "            label=model_name, color=colors_dict[model_name], markersize=8)\n",
    "ax.set_xlabel('Input SNR (dB)', fontsize=12)\n",
    "ax.set_ylabel('PSNR Improvement (dB)', fontsize=12)\n",
    "ax.set_title('PSNR Improvement', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Grouped Bar Chart\n",
    "ax = axes[1, 1]\n",
    "x_pos = np.arange(len(snr_levels))\n",
    "width = 0.15\n",
    "for i, model_name in enumerate(loaded_models.keys()):\n",
    "    acc_values = [snr_results[model_name][str(snr)]['classification']['accuracy'] * 100 \n",
    "                  for snr in snr_levels if str(snr) in snr_results[model_name]]\n",
    "    ax.bar(x_pos + i * width, acc_values, width, \n",
    "           label=model_name, color=colors_dict[model_name], alpha=0.8)\n",
    "ax.set_xlabel('Input SNR (dB)', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Accuracy by SNR (Bar)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos + width * 2.5)\n",
    "ax.set_xticklabels(snr_levels)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 6: Heatmap - PSNR by Model and SNR\n",
    "ax = axes[1, 2]\n",
    "heatmap_data = []\n",
    "for model_name in loaded_models.keys():\n",
    "    psnr_values = [snr_results[model_name][str(snr)]['restoration']['psnr'] \n",
    "                   for snr in snr_levels if str(snr) in snr_results[model_name]]\n",
    "    heatmap_data.append(psnr_values)\n",
    "heatmap_df = pd.DataFrame(heatmap_data, \n",
    "                          index=list(loaded_models.keys()),\n",
    "                          columns=[f'{snr}dB' for snr in snr_levels])\n",
    "sns.heatmap(heatmap_df, annot=True, fmt='.2f', cmap='RdYlGn', ax=ax, cbar_kws={'label': 'PSNR (dB)'})\n",
    "ax.set_title('PSNR Heatmap', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Input SNR', fontsize=12)\n",
    "ax.set_ylabel('Model', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/snr_level_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization saved: results/snr_level_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze Sequential vs MTL\n",
    "print(\"\\n[Sequential vs MTL Comparison]\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "seq_models = [k for k in loaded_models.keys() if 'Sequential' in k]\n",
    "mtl_models = [k for k in loaded_models.keys() if 'MTL' in k]\n",
    "\n",
    "# By Noise Type\n",
    "print(\"\\nBy Noise Type:\")\n",
    "for noise_type in noise_types:\n",
    "    seq_psnr = [noise_results[m][noise_type]['restoration']['psnr'] \n",
    "                for m in seq_models if noise_type in noise_results[m]]\n",
    "    mtl_psnr = [noise_results[m][noise_type]['restoration']['psnr'] \n",
    "                for m in mtl_models if noise_type in noise_results[m]]\n",
    "    \n",
    "    seq_acc = [noise_results[m][noise_type]['classification']['accuracy'] \n",
    "               for m in seq_models if noise_type in noise_results[m]]\n",
    "    mtl_acc = [noise_results[m][noise_type]['classification']['accuracy'] \n",
    "               for m in mtl_models if noise_type in noise_results[m]]\n",
    "    \n",
    "    print(f\"\\n  {noise_type.upper()}:\")\n",
    "    print(f\"    Sequential - PSNR: {np.mean(seq_psnr):.2f}±{np.std(seq_psnr):.2f}dB, \"\n",
    "          f\"Acc: {np.mean(seq_acc):.4f}±{np.std(seq_acc):.4f}\")\n",
    "    print(f\"    MTL        - PSNR: {np.mean(mtl_psnr):.2f}±{np.std(mtl_psnr):.2f}dB, \"\n",
    "          f\"Acc: {np.mean(mtl_acc):.4f}±{np.std(mtl_acc):.4f}\")\n",
    "    print(f\"    Difference - PSNR: {np.mean(seq_psnr) - np.mean(mtl_psnr):+.2f}dB, \"\n",
    "          f\"Acc: {np.mean(seq_acc) - np.mean(mtl_acc):+.4f}\")\n",
    "\n",
    "# By SNR Level\n",
    "print(\"\\n\\nBy SNR Level:\")\n",
    "for snr in snr_levels:\n",
    "    snr_key = str(snr)\n",
    "    seq_psnr = [snr_results[m][snr_key]['restoration']['psnr'] \n",
    "                for m in seq_models if snr_key in snr_results[m]]\n",
    "    mtl_psnr = [snr_results[m][snr_key]['restoration']['psnr'] \n",
    "                for m in mtl_models if snr_key in snr_results[m]]\n",
    "    \n",
    "    seq_acc = [snr_results[m][snr_key]['classification']['accuracy'] \n",
    "               for m in seq_models if snr_key in snr_results[m]]\n",
    "    mtl_acc = [snr_results[m][snr_key]['classification']['accuracy'] \n",
    "               for m in mtl_models if snr_key in snr_results[m]]\n",
    "    \n",
    "    print(f\"\\n  {snr}dB:\")\n",
    "    print(f\"    Sequential - PSNR: {np.mean(seq_psnr):.2f}±{np.std(seq_psnr):.2f}dB, \"\n",
    "          f\"Acc: {np.mean(seq_acc):.4f}±{np.std(seq_acc):.4f}\")\n",
    "    print(f\"    MTL        - PSNR: {np.mean(mtl_psnr):.2f}±{np.std(mtl_psnr):.2f}dB, \"\n",
    "          f\"Acc: {np.mean(mtl_acc):.4f}±{np.std(mtl_acc):.4f}\")\n",
    "    print(f\"    Difference - PSNR: {np.mean(seq_psnr) - np.mean(mtl_psnr):+.2f}dB, \"\n",
    "          f\"Acc: {np.mean(seq_acc) - np.mean(mtl_acc):+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "findings = []\n",
    "\n",
    "# Finding 1: Best model overall\n",
    "all_psnr = [(m, np.mean([noise_results[m][nt]['restoration']['psnr'] \n",
    "                          for nt in noise_types if nt in noise_results[m]]))\n",
    "            for m in loaded_models.keys()]\n",
    "best_psnr_model = max(all_psnr, key=lambda x: x[1])\n",
    "findings.append(f\"1. Best Restoration Model: {best_psnr_model[0]} (PSNR: {best_psnr_model[1]:.2f}dB)\")\n",
    "\n",
    "all_acc = [(m, np.mean([noise_results[m][nt]['classification']['accuracy'] \n",
    "                        for nt in noise_types if nt in noise_results[m]]))\n",
    "           for m in loaded_models.keys()]\n",
    "best_acc_model = max(all_acc, key=lambda x: x[1])\n",
    "findings.append(f\"2. Best Classification Model: {best_acc_model[0]} (Accuracy: {best_acc_model[1]:.4f})\")\n",
    "\n",
    "# Finding 2: Hardest noise type\n",
    "noise_difficulty = {}\n",
    "for noise_type in noise_types:\n",
    "    avg_psnr = np.mean([noise_results[m][noise_type]['restoration']['psnr'] \n",
    "                        for m in loaded_models.keys() if noise_type in noise_results[m]])\n",
    "    noise_difficulty[noise_type] = avg_psnr\n",
    "hardest_noise = min(noise_difficulty.items(), key=lambda x: x[1])\n",
    "findings.append(f\"3. Most Challenging Noise: {hardest_noise[0].upper()} (Avg PSNR: {hardest_noise[1]:.2f}dB)\")\n",
    "\n",
    "# Finding 3: SNR sensitivity\n",
    "snr_improvements = {}\n",
    "for model_name in loaded_models.keys():\n",
    "    psnr_at_30 = snr_results[model_name]['-30']['restoration']['psnr']\n",
    "    psnr_at_10 = snr_results[model_name]['-10']['restoration']['psnr']\n",
    "    improvement = psnr_at_10 - psnr_at_30\n",
    "    snr_improvements[model_name] = improvement\n",
    "most_sensitive = max(snr_improvements.items(), key=lambda x: x[1])\n",
    "findings.append(f\"4. Most SNR-Sensitive Model: {most_sensitive[0]} (+{most_sensitive[1]:.2f}dB improvement from -30dB to -10dB)\")\n",
    "\n",
    "# Finding 4: Sequential vs MTL\n",
    "seq_avg_psnr = np.mean([all_psnr[i][1] for i, m in enumerate(loaded_models.keys()) if 'Sequential' in m])\n",
    "mtl_avg_psnr = np.mean([all_psnr[i][1] for i, m in enumerate(loaded_models.keys()) if 'MTL' in m])\n",
    "seq_avg_acc = np.mean([all_acc[i][1] for i, m in enumerate(loaded_models.keys()) if 'Sequential' in m])\n",
    "mtl_avg_acc = np.mean([all_acc[i][1] for i, m in enumerate(loaded_models.keys()) if 'MTL' in m])\n",
    "\n",
    "findings.append(f\"5. Sequential Models: Avg PSNR={seq_avg_psnr:.2f}dB, Avg Acc={seq_avg_acc:.4f}\")\n",
    "findings.append(f\"   MTL Models: Avg PSNR={mtl_avg_psnr:.2f}dB, Avg Acc={mtl_avg_acc:.4f}\")\n",
    "findings.append(f\"   Trade-off: Sequential +{seq_avg_psnr - mtl_avg_psnr:.2f}dB PSNR, MTL +{mtl_avg_acc - seq_avg_acc:.4f} Accuracy\")\n",
    "\n",
    "# Print findings\n",
    "for finding in findings:\n",
    "    print(finding)\n",
    "\n",
    "# Save findings\n",
    "with open('results/key_findings.txt', 'w') as f:\n",
    "    f.write(\"KEY FINDINGS\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    for finding in findings:\n",
    "        f.write(finding + \"\\n\")\n",
    "\n",
    "print(\"\\n✓ Key findings saved: results/key_findings.txt\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
