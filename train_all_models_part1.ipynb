{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 모델 학습 - CIFAR-10 Denoising & Classification\n",
    "\n",
    "## 모델 라인업\n",
    "### Sequential Models (2-stage)\n",
    "1. **Sequential BAM**: Dense, BAM 양방향 연상\n",
    "2. **Sequential CAE**: Conv, No skip connection\n",
    "3. **Sequential U-Net**: Conv, Skip connection\n",
    "\n",
    "### MTL Models (1-stage)\n",
    "4. **MTL BAM**: Dense, BAM 양방향 연상\n",
    "5. **MTL CAE**: Conv, No skip connection\n",
    "6. **MTL U-Net**: Conv, Skip connection\n",
    "\n",
    "## 학습 설정\n",
    "- **데이터**: 150,000장 (3 noise types × 5 SNR levels)\n",
    "- **Epochs**: 200\n",
    "- **Batch size**: 128\n",
    "- **Validation split**: 20% (120K train / 30K val)\n",
    "- **Early stopping**: patience=30\n",
    "- **LR schedule**: Exponential decay (initial → 10% at epoch 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 환경 설정 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# GPU 메모리 설정 (3070 Ti 8GB)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # GPU 메모리 제한 (8GB)\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=8192)]\n",
    "        )\n",
    "        print(\"✓ GPU memory growth enabled and limited to 8GB\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"⚠ No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading augmented data...\")\n",
    "\n",
    "# Train data\n",
    "x_train_augmented = np.load('data/x_train_augmented.npy')\n",
    "y_train_augmented = np.load('data/y_train_augmented.npy')\n",
    "x_train_clean = np.load('data/x_train_clean.npy')\n",
    "train_noise_info = pd.read_csv('data/train_noise_info.csv')\n",
    "\n",
    "# Test data\n",
    "x_test_augmented = np.load('data/x_test_augmented.npy')\n",
    "y_test_augmented = np.load('data/y_test_augmented.npy')\n",
    "x_test_clean = np.load('data/x_test_clean.npy')\n",
    "test_noise_info = pd.read_csv('data/test_noise_info.csv')\n",
    "\n",
    "print(f\"✓ Train shape: {x_train_augmented.shape}\")\n",
    "print(f\"✓ Test shape: {x_test_augmented.shape}\")\n",
    "print(f\"✓ Train clean reference shape: {x_train_clean.shape}\")\n",
    "print(f\"✓ Test clean reference shape: {x_test_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BAM 모델용 데이터 준비 (Flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAM 모델은 flattened input 필요\n",
    "print(\"Preparing flattened data for BAM models...\")\n",
    "\n",
    "# Train data flatten\n",
    "x_train_flat = x_train_augmented.reshape(x_train_augmented.shape[0], -1)\n",
    "x_train_clean_flat = np.repeat(x_train_clean, 3, axis=0).reshape(-1, 32*32*3)  # 원본 데이터를 3배 복제\n",
    "\n",
    "# Test data flatten\n",
    "x_test_flat = x_test_augmented.reshape(x_test_augmented.shape[0], -1)\n",
    "x_test_clean_flat = np.repeat(x_test_clean, 3, axis=0).reshape(-1, 32*32*3)\n",
    "\n",
    "print(f\"✓ Train flat shape: {x_train_flat.shape}\")\n",
    "print(f\"✓ Train clean flat shape: {x_train_clean_flat.shape}\")\n",
    "print(f\"✓ Test flat shape: {x_test_flat.shape}\")\n",
    "print(f\"✓ Test clean flat shape: {x_test_clean_flat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 학습 설정 및 콜백 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 파라미터\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "INITIAL_LR = 1e-3\n",
    "FINAL_LR = INITIAL_LR * 0.1  # 10%\n",
    "\n",
    "# Exponential decay rate 계산\n",
    "DECAY_RATE = (FINAL_LR / INITIAL_LR) ** (1 / EPOCHS)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Validation split: {VALIDATION_SPLIT} ({int(len(x_train_augmented)*VALIDATION_SPLIT):,} samples)\")\n",
    "print(f\"  Initial LR: {INITIAL_LR}\")\n",
    "print(f\"  Final LR: {FINAL_LR}\")\n",
    "print(f\"  Decay rate: {DECAY_RATE:.6f}\")\n",
    "\n",
    "# 디렉토리 생성\n",
    "os.makedirs('weights', exist_ok=True)\n",
    "os.makedirs('history', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "def get_callbacks(model_name, monitor='val_loss', patience=30):\n",
    "    \"\"\"\n",
    "    학습 콜백 생성\n",
    "    \n",
    "    Args:\n",
    "        model_name: 모델 이름\n",
    "        monitor: 모니터링할 메트릭\n",
    "        patience: Early stopping patience\n",
    "    \"\"\"\n",
    "    callbacks = []\n",
    "    \n",
    "    # 1. Early Stopping\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=monitor,\n",
    "        patience=patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(early_stopping)\n",
    "    \n",
    "    # 2. Model Checkpoint\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f'weights/{model_name}_best.keras',\n",
    "        monitor=monitor,\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(checkpoint)\n",
    "    \n",
    "    # 3. Learning Rate Scheduler (Exponential Decay)\n",
    "    def lr_schedule(epoch, lr):\n",
    "        return INITIAL_LR * (DECAY_RATE ** epoch)\n",
    "    \n",
    "    lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule, verbose=0)\n",
    "    callbacks.append(lr_scheduler)\n",
    "    \n",
    "    # 4. CSV Logger\n",
    "    csv_logger = keras.callbacks.CSVLogger(\n",
    "        f'logs/{model_name}_training.csv',\n",
    "        append=False\n",
    "    )\n",
    "    callbacks.append(csv_logger)\n",
    "    \n",
    "    # 5. TensorBoard\n",
    "    tensorboard = keras.callbacks.TensorBoard(\n",
    "        log_dir=f'logs/{model_name}',\n",
    "        histogram_freq=0,\n",
    "        write_graph=True\n",
    "    )\n",
    "    callbacks.append(tensorboard)\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "print(\"\\n✓ Callbacks configured\")\n",
    "print(f\"  - Early stopping (patience={30})\")\n",
    "print(f\"  - Model checkpoint\")\n",
    "print(f\"  - LR scheduler (exponential decay)\")\n",
    "print(f\"  - CSV logger\")\n",
    "print(f\"  - TensorBoard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 메모리 정리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"\n",
    "    메모리 정리\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Clearing memory...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Keras session 정리\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    # Garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # GPU 메모리 정리 시도\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        try:\n",
    "            tf.keras.backend.clear_session()\n",
    "            print(\"✓ GPU memory cleared\")\n",
    "        except:\n",
    "            print(\"⚠ Could not clear GPU memory explicitly\")\n",
    "    \n",
    "    print(\"✓ Memory cleared\\n\")\n",
    "\n",
    "def save_history(history, model_name):\n",
    "    \"\"\"\n",
    "    학습 히스토리 저장\n",
    "    \"\"\"\n",
    "    with open(f'history/{model_name}_history.pkl', 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    print(f\"✓ History saved: history/{model_name}_history.pkl\")\n",
    "\n",
    "def save_results(results, model_name):\n",
    "    \"\"\"\n",
    "    평가 결과 저장\n",
    "    \"\"\"\n",
    "    with open(f'results/{model_name}_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"✓ Results saved: results/{model_name}_results.json\")\n",
    "\n",
    "print(\"✓ Utility functions defined\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
