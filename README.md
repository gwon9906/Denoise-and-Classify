# CIFAR-10 Denoising & Classification - MTL vs Sequential Models

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

ì´ˆì € SNR í™˜ê²½(-30~-10dB)ì—ì„œ ë…¸ì´ì¦ˆ ì œê±° ë° ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” 6ê°œì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¹„êµ ì—°êµ¬

### ë¹„êµ ëª¨ë¸

#### Sequential Models (2-stage)
1. **Sequential BAM**: Dense layers + BAM ì–‘ë°©í–¥ ì—°ìƒ
2. **Sequential CAE**: Convolutional layers (Skip connection âŒ)
3. **Sequential U-Net**: Convolutional layers + Skip connection âœ…

#### MTL Models (1-stage, End-to-End)
4. **MTL BAM**: Dense layers + BAM ì–‘ë°©í–¥ ì—°ìƒ
5. **MTL CAE**: Convolutional layers (Skip connection âŒ)
6. **MTL U-Net**: Convolutional layers + Skip connection âœ…

---

## ğŸ¯ ì‹¤í—˜ ì„¤ê³„

### ë°ì´í„°ì…‹
- **Base**: CIFAR-10 (50,000 train / 10,000 test)
- **ì¦ê°• í›„**: 150,000 train / 30,000 test

#### ë…¸ì´ì¦ˆ êµ¬ì„± (ê· ë“± ë¶„í¬)
- **3ê°€ì§€ ë…¸ì´ì¦ˆ íƒ€ì…** (ê° 50,000ì¥):
  - Gaussian noise
  - Salt & Pepper noise
  - Burst noise (3ì¢…ë¥˜: Dead Pixels, Column/Row Defects, Block Bursts)

- **5ê°€ì§€ SNR ë ˆë²¨** (ê° 10,000ì¥):
  - -30 dB
  - -25 dB
  - -20 dB
  - -15 dB
  - -10 dB

### í•™ìŠµ ì„¤ì •
```python
Epochs: 200
Batch size: 128
Validation split: 20% (120K train / 30K val)
Early stopping: patience=30
Initial LR: 1e-3
Final LR: 1e-4 (10%, Exponential decay)
Loss: MAE (restoration), Categorical CrossEntropy (classification)
```

### Loss Weights (MTL ëª¨ë¸)
- **BAM & CAE**: recon_weight=0.7, cls_weight=0.3
- **U-Net**: recon_weight=0.6, cls_weight=0.4

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
.
â”œâ”€â”€ data/                           # ì¦ê°•ëœ ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ x_train_augmented.npy       # ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ (150K)
â”‚   â”œâ”€â”€ y_train_augmented.npy       # ë ˆì´ë¸”
â”‚   â”œâ”€â”€ x_train_clean.npy           # ì›ë³¸ ì´ë¯¸ì§€ (ë³µì› íƒ€ê²Ÿ)
â”‚   â”œâ”€â”€ x_test_augmented.npy        # í…ŒìŠ¤íŠ¸ ë…¸ì´ì¦ˆ (30K)
â”‚   â”œâ”€â”€ y_test_augmented.npy
â”‚   â”œâ”€â”€ x_test_clean.npy
â”‚   â”œâ”€â”€ train_noise_info.csv        # ë…¸ì´ì¦ˆ ë©”íƒ€ì •ë³´
â”‚   â””â”€â”€ test_noise_info.csv
â”‚
â”œâ”€â”€ models/                         # ëª¨ë¸ ì •ì˜ íŒŒì¼ë“¤
â”‚   â”œâ”€â”€ bam_sequential.py
â”‚   â”œâ”€â”€ bam_mtl.py
â”‚   â”œâ”€â”€ cae_sequential.py
â”‚   â”œâ”€â”€ cae_mtl.py
â”‚   â”œâ”€â”€ unet_sequential.py
â”‚   â””â”€â”€ unet_mtl.py
â”‚
â”œâ”€â”€ weights/                        # í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â”œâ”€â”€ sequential_bam_denoise.keras
â”‚   â”œâ”€â”€ sequential_bam_classification.keras
â”‚   â”œâ”€â”€ sequential_cae_restore.keras
â”‚   â”œâ”€â”€ sequential_cae_classification.keras
â”‚   â”œâ”€â”€ sequential_unet_restore.keras
â”‚   â”œâ”€â”€ sequential_unet_classification.keras
â”‚   â”œâ”€â”€ mtl_bam.keras
â”‚   â”œâ”€â”€ mtl_cae.keras
â”‚   â””â”€â”€ mtl_unet.keras
â”‚
â”œâ”€â”€ history/                        # í•™ìŠµ íˆìŠ¤í† ë¦¬
â”‚   â”œâ”€â”€ sequential_bam_stage1_history.pkl
â”‚   â”œâ”€â”€ sequential_bam_stage2_history.pkl
â”‚   â”œâ”€â”€ mtl_bam_history.pkl
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ results/                        # í‰ê°€ ê²°ê³¼
â”‚   â”œâ”€â”€ sequential_bam_results.json
â”‚   â”œâ”€â”€ mtl_bam_results.json
â”‚   â”œâ”€â”€ comprehensive_results.csv
â”‚   â”œâ”€â”€ noise_type_results.json      # NEW: ë…¸ì´ì¦ˆ íƒ€ì…ë³„ ê²°ê³¼
â”‚   â”œâ”€â”€ snr_level_results.json       # NEW: SNR ë ˆë²¨ë³„ ê²°ê³¼
â”‚   â”œâ”€â”€ noise_type_summary.csv       # NEW: ë…¸ì´ì¦ˆ ìš”ì•½
â”‚   â”œâ”€â”€ snr_level_summary.csv        # NEW: SNR ìš”ì•½
â”‚   â”œâ”€â”€ key_findings.txt             # NEW: í•µì‹¬ ë°œê²¬ì‚¬í•­
â”‚   â”œâ”€â”€ performance_comparison.png
â”‚   â”œâ”€â”€ training_curves_mtl.png
â”‚   â”œâ”€â”€ noise_type_analysis.png      # NEW: ë…¸ì´ì¦ˆ ë¶„ì„ ì°¨íŠ¸
â”‚   â””â”€â”€ snr_level_analysis.png       # NEW: SNR ë¶„ì„ ì°¨íŠ¸
â”‚
â”œâ”€â”€ logs/                           # í•™ìŠµ ë¡œê·¸ (TensorBoard + CSV)
â”‚   â”œâ”€â”€ sequential_bam_stage1_training.csv
â”‚   â”œâ”€â”€ mtl_bam/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data_preprocessing.ipynb        # ë°ì´í„° ì¦ê°•
â”œâ”€â”€ train_all_models_part1.ipynb   # í•™ìŠµ Part 1 (ì„¤ì •)
â”œâ”€â”€ train_all_models_part2.ipynb   # í•™ìŠµ Part 2 (Sequential)
â”œâ”€â”€ train_all_models_part3.ipynb   # í•™ìŠµ Part 3 (MTL)
â”œâ”€â”€ train_all_models_part4.ipynb   # í•™ìŠµ Part 4 (ì „ì²´ ë¹„êµ)
â”œâ”€â”€ train_all_models_part5.ipynb   # í•™ìŠµ Part 5 (ìƒì„¸ ë¶„ì„)
â”œâ”€â”€ utils.py                        # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â””â”€â”€ README.md                       # ì´ íŒŒì¼
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. ë°ì´í„° ì „ì²˜ë¦¬
```bash
jupyter notebook data_preprocessing.ipynb
```
- CIFAR-10 ë°ì´í„°ë¥¼ 150,000ì¥ìœ¼ë¡œ ì¦ê°•
- 3ê°€ì§€ ë…¸ì´ì¦ˆ Ã— 5ê°€ì§€ SNR ë ˆë²¨
- `data/` í´ë”ì— ì €ì¥

### 2. ëª¨ë¸ í•™ìŠµ (ìˆœì°¨ ì‹¤í–‰)
```bash
jupyter notebook train_all_models_part1.ipynb  # í™˜ê²½ ì„¤ì •
jupyter notebook train_all_models_part2.ipynb  # Sequential ëª¨ë¸
jupyter notebook train_all_models_part3.ipynb  # MTL ëª¨ë¸
jupyter notebook train_all_models_part4.ipynb  # ì „ì²´ ê²°ê³¼ ë¹„êµ
jupyter notebook train_all_models_part5.ipynb  # ìƒì„¸ ë¶„ì„ (ë…¸ì´ì¦ˆ/SNRë³„)
```

ê° Partê°€ ì™„ë£Œë˜ë©´ ìë™ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•˜ê³  ë‹¤ìŒ Partë¡œ ì§„í–‰

---

## ğŸ’» ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´
- **CPU**: Intel i7-12700K (ë˜ëŠ” ë™ê¸‰)
- **GPU**: NVIDIA RTX 3070 Ti 8GB
- **RAM**: 16GB ì´ìƒ ê¶Œì¥
- **Storage**: 20GB ì´ìƒ (ë°ì´í„° + ëª¨ë¸ ê°€ì¤‘ì¹˜)

### ì†Œí”„íŠ¸ì›¨ì–´
```
Python >= 3.8
TensorFlow >= 2.10
NumPy
Pandas
Matplotlib
Seaborn
tqdm
```

### ì„¤ì¹˜
```bash
pip install tensorflow numpy pandas matplotlib seaborn tqdm
```

---

## ğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­

### Restoration (ë³µì›)
- **MSE** (Mean Squared Error)
- **MAE** (Mean Absolute Error)
- **PSNR** (Peak Signal-to-Noise Ratio, dB)

### Classification (ë¶„ë¥˜)
- **Accuracy**
- **Top-3 Accuracy**

---

## ğŸ”¬ ì‹¤í—˜ ê²°ê³¼ ì˜ˆì‹œ

ê²°ê³¼ëŠ” `results/comprehensive_results.csv`ì— ì €ì¥ë©ë‹ˆë‹¤:

| Model | Type | Architecture | Recon MSE | Recon PSNR (dB) | Classification Acc |
|-------|------|-------------|-----------|-----------------|-------------------|
| sequential_bam | Sequential | BAM | 0.0234 | 16.31 | 0.7234 |
| sequential_cae | Sequential | CAE | 0.0198 | 17.03 | 0.7556 |
| sequential_unet | Sequential | UNET | 0.0176 | 17.54 | 0.7823 |
| mtl_bam | MTL | BAM | 0.0245 | 16.11 | 0.7345 |
| mtl_cae | MTL | CAE | 0.0211 | 16.76 | 0.7612 |
| mtl_unet | MTL | UNET | 0.0189 | 17.24 | 0.7734 |

---

## ğŸ“ˆ ì‹œê°í™”

### 1. ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ (Part 4)
`results/performance_comparison.png`
- Reconstruction MSE
- Reconstruction PSNR
- Classification Accuracy
- PSNR vs Accuracy Trade-off (Scatter plot)

### 2. í•™ìŠµ ê³¡ì„  (Part 4)
`results/training_curves_mtl.png`
- Total Loss
- Reconstruction Loss
- Classification Loss
- Reconstruction MAE
- Classification Accuracy
- Learning Rate Schedule

### 3. ë…¸ì´ì¦ˆ íƒ€ì…ë³„ ë¶„ì„ (Part 5) ğŸ†•
`results/noise_type_analysis.png`
- PSNR by Noise Type (Line & Bar charts)
- MSE by Noise Type
- Classification Accuracy by Noise Type
- PSNR Heatmap (Model Ã— Noise Type)

### 4. SNR ë ˆë²¨ë³„ ë¶„ì„ (Part 5) ğŸ†•
`results/snr_level_analysis.png`
- PSNR vs Input SNR (Line chart)
- MSE vs Input SNR
- Classification Accuracy vs Input SNR
- PSNR Improvement over SNR
- PSNR Heatmap (Model Ã— SNR Level)

---

## ğŸ“ ì—°êµ¬ í¬ì¸íŠ¸

### ë¹„êµ ë¶„ì„
1. **Sequential vs MTL**: 2-stage vs End-to-End í•™ìŠµ
2. **Architecture**: Dense(BAM) vs Conv(CAE) vs Conv+Skip(U-Net)
3. **Special Mechanism**: BAM ì–‘ë°©í–¥ ì—°ìƒì˜ íš¨ê³¼
4. **Noise Robustness**: ë…¸ì´ì¦ˆ íƒ€ì…ë³„ ì„±ëŠ¥ ì°¨ì´ ğŸ†•
5. **SNR Sensitivity**: SNR ë ˆë²¨ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ğŸ†•

### ìƒì„¸ ë¶„ì„ í•­ëª© (Part 5) ğŸ†•
- **By Noise Type**: ê° ë…¸ì´ì¦ˆ(Gaussian, S&P, Burst)ë³„ ëª¨ë¸ ì„±ëŠ¥
- **By SNR Level**: ê° SNR(-30~-10dB)ë³„ ëª¨ë¸ ì„±ëŠ¥
- **Sequential vs MTL**: ì¡°ê±´ë³„ ìƒì„¸ ë¹„êµ
- **Statistical Analysis**: í‰ê· , í‘œì¤€í¸ì°¨, ê°œì„ ë„
- **Key Findings**: í•µì‹¬ ë°œê²¬ì‚¬í•­ ìë™ ì¶”ì¶œ

### ê¸°ëŒ€ ê²°ê³¼
- **U-Net**: Skip connectionìœ¼ë¡œ ìµœê³ ì˜ ë³µì› ì„±ëŠ¥ ì˜ˆìƒ
- **MTL**: End-to-End í•™ìŠµìœ¼ë¡œ ë¶„ë¥˜ ì„±ëŠ¥ ìš°ìˆ˜ ì˜ˆìƒ
- **BAM**: ì–‘ë°©í–¥ ì—°ìƒ ë©”ì»¤ë‹ˆì¦˜ì˜ íš¨ê³¼ ê²€ì¦
- **Noise-specific**: Burst ë…¸ì´ì¦ˆì—ì„œ ëª¨ë¸ ê°„ ì„±ëŠ¥ ì°¨ì´ í´ ê²ƒìœ¼ë¡œ ì˜ˆìƒ ğŸ†•
- **SNR-dependent**: ë†’ì€ SNRì¼ìˆ˜ë¡ MTLì˜ ì´ì  ì¦ê°€ ì˜ˆìƒ ğŸ†•

---

## âš™ï¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### Noise Configuration
`data_preprocessing.ipynb`ì—ì„œ ìˆ˜ì •:
```python
snr_levels = [-30, -25, -20, -15, -10]  # SNR ë ˆë²¨
noise_types = ['gaussian', 'sp', 'burst']  # ë…¸ì´ì¦ˆ íƒ€ì…
```

### Training Hyperparameters
`train_all_models_part1.ipynb`ì—ì„œ ìˆ˜ì •:
```python
EPOCHS = 200
BATCH_SIZE = 128
VALIDATION_SPLIT = 0.2
INITIAL_LR = 1e-3
```

### Loss Weights
ê° ëª¨ë¸ ìƒì„± ì‹œ:
```python
# MTL ëª¨ë¸
mtl_bam = MTLBAM(recon_weight=0.7, cls_weight=0.3)
mtl_unet = UNetMTL(recon_weight=0.6, cls_weight=0.4)
```

---

## ğŸ› ë¬¸ì œ í•´ê²°

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# utils.py ë˜ëŠ” Part1ì—ì„œ ë©”ëª¨ë¦¬ ì œí•œ ì¡°ì •
setup_gpu_memory(memory_limit_mb=6144)  # 8GB â†’ 6GB
```

### í•™ìŠµ ì¤‘ ì»¤ë„ í¬ë˜ì‹œ
- Batch size ê°ì†Œ: `BATCH_SIZE = 64`
- Early stopping patience ê°ì†Œ: `patience=20`
- ê° Part ì‹¤í–‰ í›„ ì»¤ë„ ì¬ì‹œì‘

---

## ğŸ“ ë…¼ë¬¸ ì‘ì„± ì‹œ ì°¸ê³ ì‚¬í•­

### ì‹¤í—˜ ì¬í˜„ì„±
- Random seed ê³ ì •: `np.random.seed(42)`
- ë°ì´í„° split ê³ ì •: ë™ì¼í•œ ì¦ê°• ë°ì´í„° ì‚¬ìš©
- ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë¡

### í†µê³„ì  ìœ ì˜ì„±
- ì—¬ëŸ¬ ë²ˆ ì‹¤í—˜ í›„ í‰ê· /í‘œì¤€í¸ì°¨ ê³„ì‚°
- T-test ë˜ëŠ” Wilcoxon test ìˆ˜í–‰

---

## ğŸ‘¥ ê¸°ì—¬ì

- í”„ë¡œì íŠ¸ ì‘ì„±ì: [Your Name]
- ì—°êµ¬ ì§€ë„: [Advisor Name]

---

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License

---

## ğŸ“§ ë¬¸ì˜

ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”.

---

## ğŸ™ ê°ì‚¬ì˜ ë§

- TensorFlow/Keras íŒ€
- CIFAR-10 ë°ì´í„°ì…‹ ì œê³µì
- Anthropic Claude (ì½”ë“œ ì‘ì„± ì§€ì›)

---

**Happy Training! ğŸš€**
