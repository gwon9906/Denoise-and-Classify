{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 모든 모델 학습 스크립트\n",
        "## 노이즈 종류별 데이터셋 구성 (5만장 * 3 = 15만장) 및 모델별 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 1: 기본 설정 및 라이브러리 임포트\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, Model, regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "# Models 폴더에서 모든 모델 import\n",
        "from models import (\n",
        "    # MTL 모델들\n",
        "    create_model as create_multitask_unet,\n",
        "    create_bam_model,\n",
        "    \n",
        "    # 단일 태스크 모델들\n",
        "    create_bam_restoration_model,\n",
        "    create_bam_classification_model,\n",
        "    \n",
        "    # 기존 모델들\n",
        "    build_cae_multitask, build_cae_restoration,\n",
        "    build_dncnn_multitask, build_dncnn_restoration,\n",
        "    build_unet_multitask, build_unet_restoration, build_unet_baseline\n",
        ")\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 2: CIFAR-10 데이터 로드 및 전처리\n",
        "print(\"--- Part 2: Loading CIFAR-10 Dataset ---\")\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n",
        "cifar10_class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "num_classes = len(cifar10_class_names)\n",
        "print(f\"CIFAR-10 data loaded. Train: {x_train.shape}, Test: {x_test.shape}\")\n",
        "\n",
        "# Z-score 정규화를 위한 통계값 계산\n",
        "MEAN = tf.constant(np.mean(x_train, axis=(0, 1, 2)), dtype=tf.float32)\n",
        "STD = tf.constant(np.std(x_train, axis=(0, 1, 2)) + 1e-6, dtype=tf.float32)\n",
        "\n",
        "def to_zscore(x):\n",
        "    return (x - MEAN) / STD\n",
        "\n",
        "def from_zscore(z):\n",
        "    return z * STD + MEAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 3: 노이즈 종류별 데이터셋 생성 (5만장 * 3 = 15만장)\n",
        "print(\"--- Part 3: Creating Noise-Specific Datasets ---\")\n",
        "\n",
        "# 데이터 증강 파이프라인\n",
        "data_augmentation_pipeline = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),\n",
        "])\n",
        "\n",
        "def augment_brightness_contrast(image):\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "    return image\n",
        "\n",
        "# 노이즈 타입별 함수들\n",
        "def add_gaussian_noise(image, snr_range=(-30, -10)):\n",
        "    \"\"\"Gaussian 노이즈 추가\"\"\"\n",
        "    image_z = to_zscore(image)\n",
        "    snr_db = tf.random.uniform([], minval=snr_range[0], maxval=snr_range[1], dtype=tf.float32)\n",
        "    sigma = tf.pow(10.0, -snr_db / 20.0)\n",
        "    noise = tf.random.normal(shape=tf.shape(image_z), mean=0.0, stddev=sigma, dtype=tf.float32)\n",
        "    noisy_z = image_z + noise\n",
        "    return noisy_z, 0  # noise_type = 0\n",
        "\n",
        "def add_salt_pepper_noise(image, amount_range=(0.05, 0.30)):\n",
        "    \"\"\"Salt & Pepper 노이즈 추가\"\"\"\n",
        "    image_z = to_zscore(image)\n",
        "    amount = tf.random.uniform([], minval=amount_range[0], maxval=amount_range[1], dtype=tf.float32)\n",
        "    u = tf.random.uniform(shape=tf.shape(image_z))\n",
        "    salt = tf.cast(u < amount * 0.5, tf.float32)\n",
        "    pepper = tf.cast(u > 1.0 - amount * 0.5, tf.float32)\n",
        "    noisy_z = image_z * (1.0 - salt - pepper) + salt\n",
        "    return noisy_z, 1  # noise_type = 1\n",
        "\n",
        "def add_burst_noise(image, size_range=(0.2, 0.4), intensity_range=(0.7, 1.0)):\n",
        "    \"\"\"Burst 노이즈 추가\"\"\"\n",
        "    image_z = to_zscore(image)\n",
        "    h, w, cch = tf.shape(image_z)[0], tf.shape(image_z)[1], tf.shape(image_z)[2]\n",
        "    size_factor = tf.random.uniform([], size_range[0], size_range[1])\n",
        "    intensity = tf.random.uniform([], intensity_range[0], intensity_range[1])\n",
        "    bh = tf.cast(tf.cast(h, tf.float32) * size_factor, tf.int32)\n",
        "    bw = tf.cast(tf.cast(w, tf.float32) * size_factor, tf.int32)\n",
        "    sy = tf.random.uniform([], maxval=tf.maximum(1, h - bh), dtype=tf.int32)\n",
        "    sx = tf.random.uniform([], maxval=tf.maximum(1, w - bw), dtype=tf.int32)\n",
        "    \n",
        "    patch = tf.random.normal([bh, bw, cch], stddev=intensity)\n",
        "    noise = tf.pad(patch, [[sy, h - sy - bh], [sx, w - sx - bw], [0, 0]])\n",
        "    mask = tf.pad(tf.ones([bh, bw, cch]), [[sy, h - sy - bh], [sx, w - sx - bw], [0, 0]])\n",
        "    noisy_z = image_z * (1.0 - mask) + (image_z + noise) * mask\n",
        "    return noisy_z, 2  # noise_type = 2\n",
        "\n",
        "def generate_noise_specific_sample(clean_image, label, noise_type):\n",
        "    \"\"\"특정 노이즈 타입으로 샘플 생성\"\"\"\n",
        "    # 1) 증강\n",
        "    aug = data_augmentation_pipeline(clean_image[tf.newaxis], training=True)\n",
        "    aug = tf.squeeze(aug, axis=0)\n",
        "    aug = augment_brightness_contrast(aug)\n",
        "    clean_aug = tf.clip_by_value(aug, 0.0, 1.0)\n",
        "    \n",
        "    # 2) 노이즈 추가\n",
        "    if noise_type == 0:\n",
        "        noisy_z, _ = add_gaussian_noise(clean_aug)\n",
        "    elif noise_type == 1:\n",
        "        noisy_z, _ = add_salt_pepper_noise(clean_aug)\n",
        "    else:  # noise_type == 2\n",
        "        noisy_z, _ = add_burst_noise(clean_aug)\n",
        "    \n",
        "    # 3) clean_z 준비\n",
        "    clean_z = to_zscore(clean_aug)\n",
        "    \n",
        "    return (noisy_z, noise_type), (clean_z, label)\n",
        "\n",
        "print(\"노이즈 함수 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 4: 노이즈 종류별 데이터셋 생성 (5만장 * 3)\n",
        "print(\"--- Part 4: Creating Noise-Specific Datasets (50k * 3) ---\")\n",
        "\n",
        "# 각 노이즈 타입별로 5만장씩 생성\n",
        "BATCH_SIZE = 64\n",
        "noise_types = ['gaussian', 'salt_pepper', 'burst']\n",
        "datasets = {}\n",
        "\n",
        "for i, noise_type in enumerate(noise_types):\n",
        "    print(f\"{noise_type} 노이즈 데이터셋 생성 중...\")\n",
        "    \n",
        "    # 각 노이즈 타입별로 5만장 생성\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    dataset = dataset.shuffle(50000).take(50000)  # 5만장 선택\n",
        "    dataset = dataset.map(\n",
        "        lambda img, lbl: generate_noise_specific_sample(img, lbl, i),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    datasets[noise_type] = dataset\n",
        "    print(f\"{noise_type} 데이터셋 크기: {len(list(dataset))} 배치\")\n",
        "\n",
        "# 테스트 데이터셋도 생성\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.map(\n",
        "    lambda img, lbl: generate_noise_specific_sample(img, lbl, 0),  # gaussian으로 테스트\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"테스트 데이터셋 크기: {len(list(test_dataset))} 배치\")\n",
        "print(\"데이터셋 생성 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 5: 모델 정의 및 학습 함수\n",
        "print(\"--- Part 5: Model Training Functions ---\")\n",
        "\n",
        "def train_model(model, train_datasets, test_dataset, model_name, epochs=20, save_path=None):\n",
        "    \"\"\"모델 학습 함수\"\"\"\n",
        "    print(f\"\\n=== {model_name} 모델 학습 시작 ===\")\n",
        "    \n",
        "    # 콜백 설정\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "        ModelCheckpoint(\n",
        "            filepath=f'best_{model_name.lower()}.keras',\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # 학습 히스토리 저장\n",
        "    histories = {}\n",
        "    \n",
        "    # 각 노이즈 타입별로 학습\n",
        "    for noise_type, dataset in train_datasets.items():\n",
        "        print(f\"\\n{noise_type} 노이즈로 학습 중...\")\n",
        "        \n",
        "        history = model.fit(\n",
        "            dataset,\n",
        "            validation_data=test_dataset,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        histories[noise_type] = history\n",
        "        \n",
        "        # 모델 저장\n",
        "        if save_path:\n",
        "            model.save(f\"{save_path}_{noise_type}.keras\")\n",
        "    \n",
        "    return histories\n",
        "\n",
        "def evaluate_model(model, test_dataset, model_name):\n",
        "    \"\"\"모델 평가 함수\"\"\"\n",
        "    print(f\"\\n=== {model_name} 모델 평가 ===\")\n",
        "    \n",
        "    # 테스트 데이터로 예측\n",
        "    predictions = model.predict(test_dataset, verbose=1)\n",
        "    \n",
        "    # 결과 출력\n",
        "    if isinstance(predictions, dict):\n",
        "        print(f\"복원 출력 형태: {predictions['restoration_output'].shape}\")\n",
        "        print(f\"분류 출력 형태: {predictions['classification_output'].shape}\")\n",
        "    else:\n",
        "        print(f\"출력 형태: {predictions.shape}\")\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "print(\"학습 함수 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 6: MTL 모델들 학습\n",
        "print(\"--- Part 6: Training MTL Models ---\")\n",
        "\n",
        "# 6-1. Multitask U-Net 학습\n",
        "print(\"\\n1. Multitask U-Net 학습\")\n",
        "mtl_unet = create_multitask_unet(input_shape=(32, 32, 3), num_classes=num_classes)\n",
        "mtl_unet.compile(\n",
        "    optimizer='adam',\n",
        "    loss={'restoration_output': 'mse', 'classification_output': 'categorical_crossentropy'},\n",
        "    loss_weights={'restoration_output': 1.0, 'classification_output': 0.1},\n",
        "    metrics={'restoration_output': ['mae'], 'classification_output': ['accuracy']}\n",
        ")\n",
        "\n",
        "mtl_unet_histories = train_model(\n",
        "    mtl_unet, datasets, test_dataset, \n",
        "    \"MTL_UNet\", epochs=20, save_path=\"models/mtl_unet\"\n",
        ")\n",
        "\n",
        "# 6-2. BAM MTL 모델 학습\n",
        "print(\"\\n2. BAM MTL 모델 학습\")\n",
        "# BAM 모델은 평탄화된 입력을 사용하므로 데이터 변환 필요\n",
        "def prepare_bam_data(dataset):\n",
        "    \"\"\"BAM 모델용 데이터 변환\"\"\"\n",
        "    def transform_batch(batch):\n",
        "        (noisy_z, noise_type), (clean_z, label) = batch\n",
        "        # 평탄화\n",
        "        noisy_flat = tf.reshape(noisy_z, [tf.shape(noisy_z)[0], -1])\n",
        "        clean_flat = tf.reshape(clean_z, [tf.shape(clean_z)[0], -1])\n",
        "        # 원-핫 인코딩\n",
        "        label_1h = tf.keras.utils.to_categorical(label, num_classes)\n",
        "        return (noisy_flat, noise_type), (clean_flat, label_1h)\n",
        "    \n",
        "    return dataset.map(transform_batch)\n",
        "\n",
        "# BAM용 데이터셋 준비\n",
        "bam_datasets = {}\n",
        "for noise_type, dataset in datasets.items():\n",
        "    bam_datasets[noise_type] = prepare_bam_data(dataset)\n",
        "\n",
        "bam_test_dataset = prepare_bam_data(test_dataset)\n",
        "\n",
        "bam_mtl = create_bam_model(input_dim=3072, latent_dim=128, num_classes=num_classes)\n",
        "bam_mtl_histories = train_model(\n",
        "    bam_mtl, bam_datasets, bam_test_dataset,\n",
        "    \"BAM_MTL\", epochs=20, save_path=\"models/bam_mtl\"\n",
        ")\n",
        "\n",
        "print(\"MTL 모델들 학습 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 7: 단일 태스크 모델들 학습\n",
        "print(\"--- Part 7: Training Single-Task Models ---\")\n",
        "\n",
        "# 7-1. BAM 복원 모델 학습\n",
        "print(\"\\n1. BAM 복원 모델 학습\")\n",
        "bam_restoration = create_bam_restoration_model(input_dim=3072, latent_dim=128)\n",
        "\n",
        "# 복원용 데이터셋 준비 (복원만)\n",
        "def prepare_restoration_data(dataset):\n",
        "    def transform_batch(batch):\n",
        "        (noisy_z, noise_type), (clean_z, label) = batch\n",
        "        noisy_flat = tf.reshape(noisy_z, [tf.shape(noisy_z)[0], -1])\n",
        "        clean_flat = tf.reshape(clean_z, [tf.shape(clean_z)[0], -1])\n",
        "        return noisy_flat, clean_flat\n",
        "    return dataset.map(transform_batch)\n",
        "\n",
        "restoration_datasets = {}\n",
        "for noise_type, dataset in datasets.items():\n",
        "    restoration_datasets[noise_type] = prepare_restoration_data(dataset)\n",
        "\n",
        "restoration_test_dataset = prepare_restoration_data(test_dataset)\n",
        "\n",
        "bam_restoration_histories = train_model(\n",
        "    bam_restoration, restoration_datasets, restoration_test_dataset,\n",
        "    \"BAM_Restoration\", epochs=20, save_path=\"models/bam_restoration\"\n",
        ")\n",
        "\n",
        "# 7-2. BAM 분류 모델 학습\n",
        "print(\"\\n2. BAM 분류 모델 학습\")\n",
        "bam_classification = create_bam_classification_model(input_dim=3072, latent_dim=128, num_classes=num_classes)\n",
        "\n",
        "# 분류용 데이터셋 준비 (분류만)\n",
        "def prepare_classification_data(dataset):\n",
        "    def transform_batch(batch):\n",
        "        (noisy_z, noise_type), (clean_z, label) = batch\n",
        "        noisy_flat = tf.reshape(noisy_z, [tf.shape(noisy_z)[0], -1])\n",
        "        label_1h = tf.keras.utils.to_categorical(label, num_classes)\n",
        "        return noisy_flat, label_1h\n",
        "    return dataset.map(transform_batch)\n",
        "\n",
        "classification_datasets = {}\n",
        "for noise_type, dataset in datasets.items():\n",
        "    classification_datasets[noise_type] = prepare_classification_data(dataset)\n",
        "\n",
        "classification_test_dataset = prepare_classification_data(test_dataset)\n",
        "\n",
        "bam_classification_histories = train_model(\n",
        "    bam_classification, classification_datasets, classification_test_dataset,\n",
        "    \"BAM_Classification\", epochs=20, save_path=\"models/bam_classification\"\n",
        ")\n",
        "\n",
        "print(\"단일 태스크 모델들 학습 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 8: 기존 모델들 학습\n",
        "print(\"--- Part 8: Training Existing Models ---\")\n",
        "\n",
        "# 8-1. CAE 모델들\n",
        "print(\"\\n1. CAE 모델들 학습\")\n",
        "cae_multitask = build_cae_multitask(input_shape=(32, 32, 3), num_classes=num_classes)\n",
        "cae_multitask.compile(\n",
        "    optimizer='adam',\n",
        "    loss={'restoration_output': 'mse', 'classification_output': 'categorical_crossentropy'},\n",
        "    loss_weights={'restoration_output': 1.0, 'classification_output': 0.1},\n",
        "    metrics={'restoration_output': ['mae'], 'classification_output': ['accuracy']}\n",
        ")\n",
        "\n",
        "cae_multitask_histories = train_model(\n",
        "    cae_multitask, datasets, test_dataset,\n",
        "    \"CAE_Multitask\", epochs=20, save_path=\"models/cae_multitask\"\n",
        ")\n",
        "\n",
        "# 8-2. DnCNN 모델들\n",
        "print(\"\\n2. DnCNN 모델들 학습\")\n",
        "dncnn_multitask = build_dncnn_multitask(input_shape=(32, 32, 3), num_classes=num_classes)\n",
        "dncnn_multitask.compile(\n",
        "    optimizer='adam',\n",
        "    loss={'restoration_output': 'mse', 'classification_output': 'categorical_crossentropy'},\n",
        "    loss_weights={'restoration_output': 1.0, 'classification_output': 0.1},\n",
        "    metrics={'restoration_output': ['mae'], 'classification_output': ['accuracy']}\n",
        ")\n",
        "\n",
        "dncnn_multitask_histories = train_model(\n",
        "    dncnn_multitask, datasets, test_dataset,\n",
        "    \"DnCNN_Multitask\", epochs=20, save_path=\"models/dncnn_multitask\"\n",
        ")\n",
        "\n",
        "# 8-3. U-Net 모델들\n",
        "print(\"\\n3. U-Net 모델들 학습\")\n",
        "unet_multitask = build_unet_multitask(input_shape=(32, 32, 3), num_classes=num_classes)\n",
        "unet_multitask.compile(\n",
        "    optimizer='adam',\n",
        "    loss={'restoration_output': 'mse', 'classification_output': 'categorical_crossentropy'},\n",
        "    loss_weights={'restoration_output': 1.0, 'classification_output': 0.1},\n",
        "    metrics={'restoration_output': ['mae'], 'classification_output': ['accuracy']}\n",
        ")\n",
        "\n",
        "unet_multitask_histories = train_model(\n",
        "    unet_multitask, datasets, test_dataset,\n",
        "    \"UNet_Multitask\", epochs=20, save_path=\"models/unet_multitask\"\n",
        ")\n",
        "\n",
        "print(\"기존 모델들 학습 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 9: 모델 평가 및 결과 시각화\n",
        "print(\"--- Part 9: Model Evaluation and Visualization ---\")\n",
        "\n",
        "# 모든 모델 평가\n",
        "models_to_evaluate = {\n",
        "    'MTL_UNet': mtl_unet,\n",
        "    'BAM_MTL': bam_mtl,\n",
        "    'BAM_Restoration': bam_restoration,\n",
        "    'BAM_Classification': bam_classification,\n",
        "    'CAE_Multitask': cae_multitask,\n",
        "    'DnCNN_Multitask': dncnn_multitask,\n",
        "    'UNet_Multitask': unet_multitask\n",
        "}\n",
        "\n",
        "evaluation_results = {}\n",
        "\n",
        "for model_name, model in models_to_evaluate.items():\n",
        "    print(f\"\\n{model_name} 평가 중...\")\n",
        "    \n",
        "    if 'BAM' in model_name and 'MTL' not in model_name:\n",
        "        # BAM 단일 태스크 모델들\n",
        "        if 'Restoration' in model_name:\n",
        "            predictions = model.predict(restoration_test_dataset, verbose=1)\n",
        "        else:  # Classification\n",
        "            predictions = model.predict(classification_test_dataset, verbose=1)\n",
        "    else:\n",
        "        # MTL 모델들\n",
        "        if 'BAM' in model_name:\n",
        "            predictions = model.predict(bam_test_dataset, verbose=1)\n",
        "        else:\n",
        "            predictions = model.predict(test_dataset, verbose=1)\n",
        "    \n",
        "    evaluation_results[model_name] = predictions\n",
        "    print(f\"{model_name} 평가 완료\")\n",
        "\n",
        "print(\"\\n모든 모델 평가 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 10: 학습 결과 시각화\n",
        "print(\"--- Part 10: Training Results Visualization ---\")\n",
        "\n",
        "# 학습 히스토리 시각화\n",
        "def plot_training_history(histories, model_name, noise_types):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle(f'{model_name} Training History', fontsize=16)\n",
        "    \n",
        "    for i, noise_type in enumerate(noise_types):\n",
        "        history = histories[noise_type]\n",
        "        \n",
        "        # Loss\n",
        "        axes[0, 0].plot(history.history['loss'], label=f'{noise_type} train')\n",
        "        axes[0, 0].plot(history.history['val_loss'], label=f'{noise_type} val')\n",
        "        axes[0, 0].set_title('Loss')\n",
        "        axes[0, 0].legend()\n",
        "        \n",
        "        # Accuracy (if available)\n",
        "        if 'accuracy' in history.history:\n",
        "            axes[0, 1].plot(history.history['accuracy'], label=f'{noise_type} train')\n",
        "            axes[0, 1].plot(history.history['val_accuracy'], label=f'{noise_type} val')\n",
        "            axes[0, 1].set_title('Accuracy')\n",
        "            axes[0, 1].legend()\n",
        "        \n",
        "        # MAE (if available)\n",
        "        if 'mae' in history.history:\n",
        "            axes[1, 0].plot(history.history['mae'], label=f'{noise_type} train')\n",
        "            axes[1, 0].plot(history.history['val_mae'], label=f'{noise_type} val')\n",
        "            axes[1, 0].set_title('MAE')\n",
        "            axes[1, 0].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 각 모델별 히스토리 시각화\n",
        "all_histories = {\n",
        "    'MTL_UNet': mtl_unet_histories,\n",
        "    'BAM_MTL': bam_mtl_histories,\n",
        "    'BAM_Restoration': bam_restoration_histories,\n",
        "    'BAM_Classification': bam_classification_histories,\n",
        "    'CAE_Multitask': cae_multitask_histories,\n",
        "    'DnCNN_Multitask': dncnn_multitask_histories,\n",
        "    'UNet_Multitask': unet_multitask_histories\n",
        "}\n",
        "\n",
        "for model_name, histories in all_histories.items():\n",
        "    if histories:  # 히스토리가 있는 경우만\n",
        "        plot_training_history(histories, model_name, noise_types)\n",
        "\n",
        "print(\"학습 결과 시각화 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 11: 최종 결과 요약\n",
        "print(\"--- Part 11: Final Results Summary ---\")\n",
        "\n",
        "print(\"\\n=== 학습 완료된 모델들 ===\")\n",
        "print(\"1. MTL 모델들:\")\n",
        "print(\"   - MTL U-Net\")\n",
        "print(\"   - BAM MTL\")\n",
        "print(\"   - CAE Multitask\")\n",
        "print(\"   - DnCNN Multitask\")\n",
        "print(\"   - U-Net Multitask\")\n",
        "\n",
        "print(\"\\n2. 단일 태스크 모델들:\")\n",
        "print(\"   - BAM Restoration\")\n",
        "print(\"   - BAM Classification\")\n",
        "\n",
        "print(\"\\n3. 데이터셋 정보:\")\n",
        "print(f\"   - 노이즈 타입: {noise_types}\")\n",
        "print(f\"   - 각 타입별 데이터: 50,000장\")\n",
        "print(f\"   - 총 학습 데이터: 150,000장\")\n",
        "print(f\"   - 테스트 데이터: 10,000장\")\n",
        "\n",
        "print(\"\\n4. 저장된 모델 파일들:\")\n",
        "import glob\n",
        "model_files = glob.glob(\"*.keras\")\n",
        "for file in sorted(model_files):\n",
        "    print(f\"   - {file}\")\n",
        "\n",
        "print(\"\\n=== 모든 모델 학습 완료! ===\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
